# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_ImageEncoder.ipynb.

# %% auto 0
__all__ = ['dls', 'visConfig', 'classifier', 'loss_func', 'VisionConfig', 'ResBlock', 'VisionEncoder', 'log',
           'vision_encoder_train', 'save_model']

# %% ../nbs/02_ImageEncoder.ipynb 5
from .data import *
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

# %% ../nbs/02_ImageEncoder.ipynb 7
dls = get_vision_classifier_dl()
for x, y in dls['valid']:
    break
x.shape, y.shape

# %% ../nbs/02_ImageEncoder.ipynb 9
from dataclasses import dataclass

@dataclass
class VisionConfig:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

    head_op_dim = 512
    nc = 10

visConfig = VisionConfig()

# %% ../nbs/02_ImageEncoder.ipynb 11
import torch.nn.functional as F

class ResBlock(nn.Module):
    """
    Residual Block with skip connection.
    Applies two convolutions with a skip connection that adds input to output.
    """
    def __init__(self, ni, nf, ks=3, stride=2):
        """
        Args:
            ni: number of input channels
            nf: number of output channels (filters)
            ks: kernel size (default 3)
            stride: stride for first conv (default 2 for downsampling)
        """
        super().__init__()
        # First conv: changes channels and spatial dims
        self.conv1 = nn.Sequential(
            nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride),
            nn.BatchNorm2d(nf))

        # Second conv: keeps channels and spatial dims constant
        self.conv2 = nn.Sequential(
            nn.Conv2d(nf, nf, ks, padding=ks//2, stride=1),
            nn.BatchNorm2d(nf))

        # Handle dimension mismatch
        self.skip = nn.Conv2d(ni, nf, 1, stride=stride) if ni != nf else nn.Identity()

    def forward(self, x):
        # Add skip connection to output of two convs
        return F.relu(self.skip(x) + self.conv2(F.relu(self.conv1(x))))

# %% ../nbs/02_ImageEncoder.ipynb 12
class VisionEncoder(nn.Module):
    """
    CNN encoder for Fashion MNIST images.
    Progressively downsamples and increases channels to create feature vector.
    """
    def __init__(self):
        super().__init__()

        self.VisionHead = nn.Sequential(
            ResBlock(1, 64, ks=7, stride=2),      # 28→14
            ResBlock(64, 128, stride=2),          # 14→7
            ResBlock(128, 256, stride=2),         # 7→4
            ResBlock(256, visConfig.head_op_dim, stride=2),         # 4→2
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten()                          # 512
        )

    def forward(self, x):
        """
        Args:
            x: input images (bs, 1, 28, 28)
        Returns:
            feature vector (bs, 512)
        """
        return self.VisionHead(x)

# %% ../nbs/02_ImageEncoder.ipynb 13
classifier = nn.Sequential(
    VisionEncoder(),
    nn.Sequential(
            nn.Linear(visConfig.head_op_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.Linear(1024, visConfig.nc)))
classifier

# %% ../nbs/02_ImageEncoder.ipynb 16
loss_func = nn.CrossEntropyLoss()

# %% ../nbs/02_ImageEncoder.ipynb 21
def log(*args):
    print(f"{args[0]}   \t{args[1]:.4f}   \t{args[2]:.4f}\t\t{args[3]:.4f}")

# %% ../nbs/02_ImageEncoder.ipynb 22
def vision_encoder_train(model, epochs=10):
    model = model.to(visConfig.device)
    optimizer = AdamW(model.parameters(), lr=visConfig.lr)

    print(f"Epoch \tTrain Loss \tValid Loss \taccurecy")


    for epoch in range(epochs):
        model.train()
        train_loss = 0

        for x, y in dls['train']:
            x, y = x.to(visConfig.device), y.to(visConfig.device)
            optimizer.zero_grad()

            with torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):
                logits = model(x)
                loss = loss_func(logits, y)

            loss.backward()

            clip_grad_norm_(model.parameters(), visConfig.max_grad_norm) # to clip gradients

            optimizer.step()

            train_loss += loss.item()

        classifier.eval()
        val_loss = 0
        total_correct = 0
        total_samples = 0
        with torch.no_grad(), torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):
            for x, y in dls['valid']:
                x, y = x.to(visConfig.device), y.to(visConfig.device)

                logits = model(x)
                loss = loss_func(logits, y)

                val_loss += loss.item()
                predicted = logits.softmax(-1).argmax(-1)
                total_correct += (predicted == y).sum().item()
                total_samples += y.size(0)

        accurecy = total_correct / total_samples
        log(epoch+1, train_loss/len(dls['train']), val_loss/len(dls['valid']), accurecy)

# %% ../nbs/02_ImageEncoder.ipynb 25
def save_model(fn='classfier.pth'):
    torch.save(classifier, path/fn)
