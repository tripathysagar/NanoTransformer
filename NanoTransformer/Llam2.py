# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_llama2.ipynb.

# %% auto 0
__all__ = ['llamaConfig', 'text_dls', 'loss_func', 'LlamaConfig', 'Embedding', 'RoPE', 'LlamaText', 'Optim', 'get_cosine_lr',
           'train']

# %% ../nbs/05_llama2.ipynb 3
from .data import *

# %% ../nbs/05_llama2.ipynb 4
from dataclasses import dataclass
import torch

@dataclass
class LlamaConfig:
    bs = 256
    seq_len = 128                # context length
    embedding_dim = 128          # dim of the embedding layer
    n_layers = 4                # no of decoder block stack on top of each other
    n_heads = 8                 # no of heads in a single decoder block
    vocab_size = len(tokenizer.vocab)
    dropout = 0.1

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

    epochs = 30
    no_kv = 1

    max_lr = 5e-4
    min_lr = 1e-4

llamaConfig = LlamaConfig()

# %% ../nbs/05_llama2.ipynb 5
text_dls = get_text_dl(bs=llamaConfig.bs, seq_len=llamaConfig.seq_len)
text_dls

# %% ../nbs/05_llama2.ipynb 6
llamaConfig.max_steps = len(text_dls['train']) * llamaConfig.epochs
llamaConfig.warm_steps = int(llamaConfig.max_steps * 0.1)

# %% ../nbs/05_llama2.ipynb 8
from torch import nn
class Embedding(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1

        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)

    def forward(self, x):           #bs * seq_len
        return self.embed(x)        #bs * seq_len * embedding_dim

# %% ../nbs/05_llama2.ipynb 10
class RoPE(nn.Module):
    def __init__(self, head_dim, seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.seq_len = seq_len

        no = torch.arange(head_dim // 2)
        theta = 1.0 / (10000 ** (2 * no / head_dim ))    # angle

        # Position indices
        positions = torch.arange(seq_len)
        angles = positions[:, None] * theta             # (seq_len, emb_dim//2)

        self.register_buffer('cos_angles', torch.cos(angles))
        self.register_buffer('sin_angles', torch.sin(angles))

    def forward(self, x):
        bs, seq_len, head_dim = x.shape # emb_dim is a name bug but it head_dim is passed

        x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:] # hf implement by deviding the emb in two half and then caluclate the rotation

        rotated_x1 = x1 * self.cos_angles[:seq_len] - x2 * self.sin_angles[:seq_len]
        rotated_x2 = x1 * self.sin_angles[:seq_len] + x2 * self.cos_angles[:seq_len]

        return  torch.cat([rotated_x1, rotated_x2], dim=-1)


# %% ../nbs/05_llama2.ipynb 42
class LlamaText(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.config = config

        self.embed = Embedding(config)
        self.blocks = nn.ModuleList(
            [DecoderBlock(config) for _ in range(config.n_layers)])
        self.final_norm = RMSNorm(config.embedding_dim)
        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)

        self._init_weights()

    def forward(self, x):
        x = self.embed(x)
        for block in self.blocks:
            x = block(x)
        return self.lm_head(self.final_norm(x))


    def _init_weights(self):
        """Init"""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                std = 0.02
                # Scale down residual projections
                if 'linear_MHA' in name or 'w2' in name:
                    std = 0.02 / ((2 * self.config.n_layers) ** 0.5)

                module.weight.data.normal_(mean=0.0, std=std)
                if module.bias is not None:
                    module.bias.data.zero_()

            elif isinstance(module, nn.Embedding):
                module.weight.data.normal_(mean=0.0, std=0.02)

    def decay_params(self):
        """
        get decay params
        """
        no_decay_params, decay_params = [], []
        for name, param in self.named_parameters():
            if 'bias' in name or 'norm' in name or 'embed' in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)

        return decay_params, no_decay_params


# %% ../nbs/05_llama2.ipynb 46
from torch.optim import AdamW
class Optim:
    def __init__(self, model, config:LlamaConfig):
        decay_params, no_decay_params = model.decay_params() # extract deacayable and non deacayable params

        self.optim = optimizer = AdamW([
            {'params': decay_params, 'weight_decay': 0.1},
            {'params': no_decay_params, 'weight_decay': 0.0}
        ], lr=config.min_lr)

    def config_lr(self, step):
        for param_group in self.optim.param_groups:
            param_group['lr'] = get_cosine_lr(step)

    def step(self): self.optim.step()

    def zero(self): self.optim.zero_grad()

# %% ../nbs/05_llama2.ipynb 47
import math
def get_cosine_lr(
    step,
    warmup_steps=llamaConfig.warm_steps,
    max_steps=llamaConfig.max_steps,
    max_lr=llamaConfig.max_lr,
    min_lr=llamaConfig.min_lr
    ):
    if step < warmup_steps:
        return min_lr + (max_lr - min_lr) * step / warmup_steps
    elif step < max_steps:
        progress = (step - warmup_steps) / (max_steps - warmup_steps)
        return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))
    else:
        return min_lr


# %% ../nbs/05_llama2.ipynb 49
from torch.nn.utils import clip_grad_norm_
loss_func = nn.CrossEntropyLoss()
def train(model):
    model = model.to(llamaConfig.device)

    optimizer = Optim(model, llamaConfig)

    step = 0
    for epoch in range(llamaConfig.epochs):
        model.train()
        train_loss, no_train = 0, 0

        for x, y in text_dls['train']: # Iterate directly over
            step += 1
            no_train += 1

            x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)           # move to device

            optimizer.config_lr(step)                                           # set lr for each step
            optimizer.zero()

            with torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):
              logits = model(x)

              loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))

            loss.backward()

            clip_grad_norm_(model.parameters(), llamaConfig.max_grad_norm) # to clip gradients

            optimizer.step()

            train_loss += loss.item()

        model.eval()
        val_loss, no_valid = 0, 0
        with torch.no_grad(), torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):
            for x, y in text_dls['valid']:
                no_valid += 1

                x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)

                logits = model(x)
                loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))

                val_loss += loss.item()

        print(f"{epoch} -> {train_loss/no_train:.4f} : {val_loss/no_valid:.4f}")


