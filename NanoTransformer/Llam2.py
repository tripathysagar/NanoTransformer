# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_llama2.ipynb.

# %% auto 0
__all__ = ['llamaConfig', 'text_dls', 'loss_func', 'LlamaConfig', 'Embedding', 'RoPE', 'RMSNorm', 'SwiGLU_FFN', 'AttentionHead',
           'GroupedQueryAttention', 'DecoderBlock', 'LlamaText', 'get_cosine_lr', 'Optim', 'train', 'generate']

# %% ../nbs/05_llama2.ipynb 3
from .data import *

# %% ../nbs/05_llama2.ipynb 4
from dataclasses import dataclass
import torch

@dataclass
class LlamaConfig:
    bs = 256
    seq_len = 128                # context length
    embedding_dim = 128          # dim of the embedding layer
    n_layers = 4                # no of decoder block stack on top of each other
    n_heads = 8                 # no of heads in a single decoder block
    vocab_size = len(tokenizer.vocab)
    dropout = 0.1

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

    epochs = 30
    no_kv = 1

    max_lr = 5e-4
    min_lr = 1e-4

llamaConfig = LlamaConfig()

# %% ../nbs/05_llama2.ipynb 5
text_dls = get_text_dl(bs=llamaConfig.bs, seq_len=llamaConfig.seq_len)
text_dls

# %% ../nbs/05_llama2.ipynb 6
llamaConfig.max_steps = len(text_dls['train']) * llamaConfig.epochs
llamaConfig.warm_steps = int(llamaConfig.max_steps * 0.1)

# %% ../nbs/05_llama2.ipynb 8
from torch import nn
class Embedding(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1

        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)

    def forward(self, x):           #bs * seq_len
        return self.embed(x)        #bs * seq_len * embedding_dim

# %% ../nbs/05_llama2.ipynb 10
class RoPE(nn.Module):
    def __init__(self, head_dim, seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.seq_len = seq_len

        no = torch.arange(head_dim // 2)
        theta = 1.0 / (10000 ** (2 * no / head_dim ))    # angle

        # Position indices
        positions = torch.arange(seq_len)
        angles = positions[:, None] * theta             # (seq_len, emb_dim//2)

        self.register_buffer('cos_angles', torch.cos(angles))
        self.register_buffer('sin_angles', torch.sin(angles))

    def forward(self, x):
        bs, seq_len, head_dim = x.shape # emb_dim is a name bug but it head_dim is passed

        x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:] # hf implement by deviding the emb in two half and then caluclate the rotation

        rotated_x1 = x1 * self.cos_angles[:seq_len] - x2 * self.sin_angles[:seq_len]
        rotated_x2 = x1 * self.sin_angles[:seq_len] + x2 * self.cos_angles[:seq_len]

        return  torch.cat([rotated_x1, rotated_x2], dim=-1)


# %% ../nbs/05_llama2.ipynb 16
class RMSNorm(nn.Module):
    def __init__(self, shape, eps=1e-5):
        super().__init__()
        self.scale_param = nn.Parameter(torch.ones(shape))
        self.eps = eps
    def forward(self, x):
        rms = torch.sqrt((x*x).mean(dim=-1, keepdim=True) + self.eps)
        return (x / rms ) * self.scale_param

# %% ../nbs/05_llama2.ipynb 19
import torch.nn.functional as F
class SwiGLU_FFN(nn.Module):
    def __init__(self, embedding_dim, dropout=0.1):
        super().__init__()
        hidden_dim = int(2 * embedding_dim * 4 / 3)
        self.w = nn.Linear(embedding_dim, hidden_dim, bias=False)
        self.v = nn.Linear(embedding_dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, embedding_dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.dropout(self.w2(F.silu(self.w(x)) * self.v(x)))

# %% ../nbs/05_llama2.ipynb 24
class AttentionHead(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()

        assert config.embedding_dim % config.n_heads == 0
        self.head_dim = config.embedding_dim // config.n_heads

        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)
        #self.K_W = nn.Linear(config.embedding_dim, self.head_dim)
        #self.V_W = nn.Linear(config.embedding_dim, self.head_dim)

        self.rope = RoPE(head_dim = self.head_dim, seq_len = config.seq_len)

        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))
        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask

        self.dropout = nn.Dropout(p = config.dropout)

    def forward(self, K, V, x): #bs * seq_len * embedding_dim

        Q = self.Q_W(x)        #bs * seq_len * head_dim

        attn = self.rope(Q) @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len

        attn += self.mask[:x.shape[1], :x.shape[1]]

        attn = torch.softmax(attn, dim=-1)

        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim

# %% ../nbs/05_llama2.ipynb 28
class GroupedQueryAttention(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.config = config
        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer
        assert config.n_heads % config.no_kv == 0                    # checikng the kv is equally divisible by no of heads
        self.head_dim = config.embedding_dim // config.n_heads

        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])

        self.K_cache = nn.Linear(config.embedding_dim, self.head_dim * config.no_kv)
        self.V_cache = nn.Linear(config.embedding_dim, self.head_dim * config.no_kv)

        self.dropout = nn.Dropout(p=config.dropout)
        self.linear_MHA = nn.Linear(config.embedding_dim, config.embedding_dim)

        self.rope = RoPE(head_dim = self.head_dim, seq_len = config.seq_len)


    def forward(self, x): #bs * seq_len * embedding_dim
        bs, seq_len, _ = x.shape

        K = self.K_cache(x)    # #bs * seq_len * (head_dim * no_kv)
        V = self.V_cache(x)    # #bs * seq_len * (head_dim * no_kv)

        K, V = K.view(bs, seq_len, self.config.no_kv, self.head_dim), V.view(bs, seq_len, self.config.no_kv, self.head_dim)
        heads = []

        for idx, head in enumerate(self.heads):
            i = idx // (self.config.n_heads // self.config.no_kv)
            heads.append(head(self.rope(K[:,:,i,:]), V[:,:,i,:], x))

        head = torch.cat(heads, dim=-1)                                #bs * seq_len * embedding_dim
        head = self.dropout(self.linear_MHA(head))                     #bs * seq_len * embedding_dim
        return head


# %% ../nbs/05_llama2.ipynb 33
class DecoderBlock(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.attn_norm = RMSNorm(config.embedding_dim)
        self.attn = GroupedQueryAttention(config)
        self.ffn_norm = RMSNorm(config.embedding_dim)
        self.ffn = SwiGLU_FFN(config.embedding_dim, config.dropout)
    def forward(self, x):
        x = x + self.attn(self.attn_norm(x))
        return x + self.ffn(self.ffn_norm(x))

# %% ../nbs/05_llama2.ipynb 42
class LlamaText(nn.Module):
    def __init__(self, config:LlamaConfig):
        super().__init__()
        self.config = config

        self.embed = Embedding(config)
        self.blocks = nn.ModuleList(
            [DecoderBlock(config) for _ in range(config.n_layers)])
        self.final_norm = RMSNorm(config.embedding_dim)
        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)

        self._init_weights()

    def forward(self, x):
        x = self.embed(x)
        for block in self.blocks:
            x = block(x)
        return self.lm_head(self.final_norm(x))


    def _init_weights(self):
        """Init"""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                std = 0.02
                # Scale down residual projections
                if 'linear_MHA' in name or 'w2' in name:
                    std = 0.02 / ((2 * self.config.n_layers) ** 0.5)

                module.weight.data.normal_(mean=0.0, std=std)
                if module.bias is not None:
                    module.bias.data.zero_()

            elif isinstance(module, nn.Embedding):
                module.weight.data.normal_(mean=0.0, std=0.02)

    def decay_params(self):
        """
        get decay params
        """
        no_decay_params, decay_params = [], []
        for name, param in self.named_parameters():
            if 'bias' in name or 'norm' in name or 'embed' in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)

        return decay_params, no_decay_params


# %% ../nbs/05_llama2.ipynb 46
import math
def get_cosine_lr(
    step,
    warmup_steps=llamaConfig.warm_steps,
    max_steps=llamaConfig.max_steps,
    max_lr=llamaConfig.max_lr,
    min_lr=llamaConfig.min_lr
    ):
    if step < warmup_steps:
        return min_lr + (max_lr - min_lr) * step / warmup_steps
    elif step < max_steps:
        progress = (step - warmup_steps) / (max_steps - warmup_steps)
        return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))
    else:
        return min_lr


# %% ../nbs/05_llama2.ipynb 48
from torch.optim import AdamW
class Optim:
    def __init__(self, model, config:LlamaConfig):
        decay_params, no_decay_params = model.decay_params() # extract deacayable and non deacayable params

        self.optim = AdamW([
            {'params': decay_params, 'weight_decay': 0.1},
            {'params': no_decay_params, 'weight_decay': 0.0}
        ], lr=config.min_lr)

    def config_lr(self, step):
        for param_group in self.optim.param_groups:
            param_group['lr'] = get_cosine_lr(step)

    def step(self): self.optim.step()

    def zero(self): self.optim.zero_grad()

# %% ../nbs/05_llama2.ipynb 49
from torch.nn.utils import clip_grad_norm_
loss_func = nn.CrossEntropyLoss()
def train(model):
    model = model.to(llamaConfig.device)

    optimizer = Optim(model, llamaConfig)

    step = 0
    for epoch in range(llamaConfig.epochs):
        model.train()
        train_loss, no_train = 0, 0

        for x, y in text_dls['train']: # Iterate directly over
            step += 1
            no_train += 1

            x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)           # move to device

            optimizer.config_lr(step)                                           # set lr for each step
            optimizer.zero()

            with torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):
              logits = model(x)

              loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))

            loss.backward()

            clip_grad_norm_(model.parameters(), llamaConfig.max_grad_norm) # to clip gradients

            optimizer.step()

            train_loss += loss.item()

        model.eval()
        val_loss, no_valid = 0, 0
        with torch.no_grad(), torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):
            for x, y in text_dls['valid']:
                no_valid += 1

                x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)

                logits = model(x)
                loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))

                val_loss += loss.item()

        print(f"{epoch} -> {train_loss/no_train:.4f} : {val_loss/no_valid:.4f}")



# %% ../nbs/05_llama2.ipynb 51
@torch.no_grad()
def generate(prompt, max_new_tokens=100, temperature=1.0):
    """
    prompt: string to start generation
    max_new_tokens: how many tokens to generate
    temperature: higher = more random, lower = more deterministic
    """
    model.eval()
    tokens = tokenizer.encode(prompt)
    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim
    tokens = tokens.to(llamaConfig.device)
    for _ in range(max_new_tokens):
        # Crop to last seq_len tokens if needed
        context = tokens if tokens.size(1) <= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]

        # Get predictions
        with torch.no_grad(), torch.autocast(device_type=llamaConfig.device, dtype=torch.bfloat16):
          logits = model(context)
        logits = logits[:, -1, :] / temperature  # Focus on last token

        # Sample next token
        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Append to sequence
        tokens = torch.cat([tokens, next_token], dim=1)

    return tokenizer.decode(tokens.squeeze().tolist())
