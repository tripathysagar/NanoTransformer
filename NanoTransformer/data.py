"""Create dataloader"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_data.ipynb.

# %% auto 0
__all__ = ['path', 'tokenizer', 'Tokenizer', 'GPTDataset', 'get_text_dl', 'show_images', 'get_vision_classifier_dl']

# %% ../nbs/00_data.ipynb 2
from fastcore.all import *
path = Path('../static')
path.mkdir(exist_ok=True)

# %% ../nbs/00_data.ipynb 5
urlsave("https://raw.githubusercontent.com/karpathy/build-nanogpt/refs/heads/master/input.txt", path) 

# %% ../nbs/00_data.ipynb 6
from typing import List
class Tokenizer:
    """
    Maps each char to unique index. It have some key attributes i.e.
    1. **voacb**: where it maped to all the char present text field
    1. **encode**: to encode given string to list of tokens
    1. **decode**: to decode given tokens to str
    1. **c2i** and **i2c**: helper function to convert char to tokens and tokens to char respectively
    """
    def __init__(self):
        self.setup_vocab()

    def setup_vocab(self):
        with open(path/'input.txt', 'r') as file:
            self.txt = file.read()

        self.vocab = sorted(list(set(list(self.txt))))
        #print(f"After reading file got the vocab of shape : {len(self.vocab)}")

    def c2i(self, ch:str) -> int:
        """
        returns index of char ch from vocab
        """
        return self.vocab.index(ch)

    def i2c(self, idx:int) -> str:
        """
        returns char from vocab given index
        """
        return self.vocab[idx]

    def encode(self, inp:str) -> List[int]:
        """
        returns the encoded string
        """
        return [self.c2i(i) for i in inp]

    def decode(self, inp:List[int]) -> str:
        """
        returns the string represntation of the
        """
        return ''.join([self.i2c(i) for i in inp])

tokenizer = Tokenizer()

# %% ../nbs/00_data.ipynb 10
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDataset(Dataset):
    def __init__(self, text, seq_len:int):
        self.text = text
        self.seq_len = seq_len
        self.encoded_text = tokenizer.encode(text)

    def __len__(self):
        return len(self.encoded_text) // self.seq_len -1

    def __getitem__(self, idx):
        inp = self.encoded_text[idx * self.seq_len : (idx + 1) * self.seq_len]
        op = self.encoded_text[idx * self.seq_len + 1 : (idx + 1) * self.seq_len + 1]
        return torch.tensor(inp), torch.tensor(op)

# %% ../nbs/00_data.ipynb 11
def get_text_dl(bs:int=64, seq_len:int=128):
    split_idx = int(len(tokenizer.txt) * 0.9)                         #split text with 9:
    train_dataset = GPTDataset(tokenizer.txt[:split_idx], seq_len )
    val_dataset = GPTDataset(tokenizer.txt[split_idx:], seq_len)

    return {
        'train': DataLoader(train_dataset, batch_size=bs, shuffle=True),
        'valid': DataLoader(val_dataset, batch_size=bs, shuffle=False)
    }

# %% ../nbs/00_data.ipynb 15
from torchvision import datasets, transforms
datasets.FashionMNIST(path, download=True)

# %% ../nbs/00_data.ipynb 17
import numpy as np
import matplotlib.pyplot as plt

def show_images(im, label=None, n=1, figsize=(2,2)):
    fig, axes = plt.subplots(1, n, figsize=figsize)
    if n == 1:
        axes = [axes]
    
    if isinstance(im, torch.Tensor):
        if im.shape[0] == 1:
            im = im.squeeze(0)  # Remove channel dim for grayscale
        elif im.shape[0] == 3:
            im = im.permute(1, 2, 0)  # Change to (H, W, C)
        im = im.numpy()


    for i, ax in enumerate(axes):
        ax.imshow(im, cmap='gray')
        if label: ax.set_title(f"Label: {label}")
        ax.axis('off')
    
    plt.tight_layout()
    plt.show()


# %% ../nbs/00_data.ipynb 24
def get_vision_classifier_dl(bs:int=64):
    im_path = path/'FashionMNIST'

    train_ds = datasets.FashionMNIST(path, train=True, transform=transforms.ToTensor())
    valid_ds = datasets.FashionMNIST(path, transform=transforms.ToTensor())

    return {
        'train': DataLoader(train_ds, batch_size=bs, shuffle=True),
        'valid': DataLoader(valid_ds, batch_size=bs)
    }
