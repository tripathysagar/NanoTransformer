# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_data.ipynb.

# %% auto 0
__all__ = ['path', 'tokenizer', 'captions', 'rand_lbl', 'Tokenizer', 'GPTDataset', 'get_text_dl', 'show_images',
           'get_vision_classifier_dl', 'MNISTDataset', 'collate_fn_multimodal', 'get_mnist_caption_dl']

# %% ../nbs/00_data.ipynb 2
from fastcore.all import *
path = Path('../static')
path.mkdir(exist_ok=True)

# %% ../nbs/00_data.ipynb 5
urlsave("https://raw.githubusercontent.com/karpathy/build-nanogpt/refs/heads/master/input.txt", path) 

# %% ../nbs/00_data.ipynb 6
from typing import List
class Tokenizer:
    """
    Maps each char to unique index. It have some key attributes i.e.
    1. **voacb**: where it maped to all the char present text field
    1. **encode**: to encode given string to list of tokens
    1. **decode**: to decode given tokens to str
    1. **c2i** and **i2c**: helper function to convert char to tokens and tokens to char respectively
    """
    def __init__(self):
        self.setup_vocab()

    def setup_vocab(self):
        with open(path/'input.txt', 'r') as file:
            self.txt = file.read()

        self.vocab = sorted(list(set(list(self.txt))))
        #print(f"After reading file got the vocab of shape : {len(self.vocab)}")

    def c2i(self, ch:str) -> int:
        """
        returns index of char ch from vocab
        """
        return self.vocab.index(ch)

    def i2c(self, idx:int) -> str:
        """
        returns char from vocab given index
        """
        return self.vocab[idx]

    def encode(self, inp:str) -> List[int]:
        """
        returns the encoded string
        """
        return [self.c2i(i) for i in inp]

    def decode(self, inp:List[int]) -> str:
        """
        returns the string represntation of the
        """
        return ''.join([self.i2c(i) for i in inp])

tokenizer = Tokenizer()

# %% ../nbs/00_data.ipynb 10
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDataset(Dataset):
    def __init__(self, text, seq_len:int):
        self.text = text
        self.seq_len = seq_len
        self.encoded_text = tokenizer.encode(text)

    def __len__(self):
        return len(self.encoded_text) // self.seq_len -1

    def __getitem__(self, idx):
        inp = self.encoded_text[idx * self.seq_len : (idx + 1) * self.seq_len]
        op = self.encoded_text[idx * self.seq_len + 1 : (idx + 1) * self.seq_len + 1]
        return torch.tensor(inp), torch.tensor(op)

# %% ../nbs/00_data.ipynb 11
def get_text_dl(bs:int=64, seq_len:int=128):
    split_idx = int(len(tokenizer.txt) * 0.9)                         #split text with 9:
    train_dataset = GPTDataset(tokenizer.txt[:split_idx], seq_len )
    val_dataset = GPTDataset(tokenizer.txt[split_idx:], seq_len)

    return {
        'train': DataLoader(train_dataset, batch_size=bs, shuffle=True),
        'valid': DataLoader(val_dataset, batch_size=bs, shuffle=False)
    }

# %% ../nbs/00_data.ipynb 15
from torchvision import datasets, transforms
datasets.FashionMNIST(path, download=True)

# %% ../nbs/00_data.ipynb 17
import numpy as np
import matplotlib.pyplot as plt

def show_images(im, label=None, figsize=(3,3)):  # Increase size
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    
    if isinstance(im, torch.Tensor):
        if im.shape[0] == 1:
            im = im.squeeze(0)
        elif im.shape[0] == 3:
            im = im.permute(1, 2, 0)
        im = im.numpy()

    ax.imshow(im, cmap='gray')
    if label: ax.set_title(f"Label: {label}")
    ax.axis('off')
    
    plt.show()  # Remove tight_layout()

# %% ../nbs/00_data.ipynb 24
def get_vision_classifier_dl(bs:int=64):
    im_path = path/'FashionMNIST'

    train_ds = datasets.FashionMNIST(im_path, train=True, transform=transforms.ToTensor())
    valid_ds = datasets.FashionMNIST(im_path, transform=transforms.ToTensor())

    return {
        'train': DataLoader(train_ds, batch_size=bs, shuffle=True),
        'valid': DataLoader(valid_ds, batch_size=bs)
    }

# %% ../nbs/00_data.ipynb 28
# captions of MNIST data
captions = {
    0: ["t-shirt", "tee shirt", "tee", "short sleeve shirt", "casual shirt", 
        "cotton tee", "basic tee", "crew neck shirt", "top", "shirt",
        "simple tee", "everyday shirt", "round neck tee", "plain tee", "short sleeve top",
        "casual top", "basic shirt", "t shirt", "crew tee", "cotton shirt"],
    
    1: ["trouser", "trousers", "pants", "slacks", "dress pants",
        "long pants", "formal pants", "casual pants", "bottoms", "leg wear",
        "straight pants", "fitted pants", "work pants", "everyday pants", "pant",
        "classic trousers", "regular pants", "full length pants", "basic pants", "standard trousers"],
    
    2: ["pullover", "sweater", "jumper", "knit sweater", "warm sweater",
        "pullover sweater", "casual sweater", "knitted top", "knitwear", "woolen top",
        "crew neck sweater", "basic pullover", "winter top", "cozy sweater", "knit top",
        "long sleeve sweater", "casual pullover", "everyday sweater", "simple sweater", "pullover top"],
    
    3: ["dress", "frock", "gown", "one piece dress", "casual dress",
        "simple dress", "everyday dress", "summer dress", "day dress", "basic dress",
        "long dress", "short dress", "women's dress", "classic dress", "plain dress",
        "fitted dress", "loose dress", "comfortable dress", "standard dress", "regular dress"],
    
    4: ["coat", "jacket", "overcoat", "long coat", "winter coat",
        "warm coat", "outer wear", "heavy jacket", "formal coat", "trench coat",
        "long jacket", "full coat", "winter jacket", "outerwear", "layering piece",
        "cold weather coat", "classic coat", "tailored coat", "button coat", "dress coat"],
    
    5: ["sandal", "sandals", "open toe shoe", "summer shoe", "casual sandal",
        "flat sandal", "beach sandal", "simple sandal", "everyday sandal", "comfortable sandal",
        "strappy sandal", "basic sandal", "slip on sandal", "warm weather shoe", "open shoe",
        "light sandal", "walking sandal", "casual footwear", "easy sandal", "relaxed sandal"],
    
    6: ["shirt", "button shirt", "dress shirt", "collared shirt", "formal shirt",
        "long sleeve shirt", "casual shirt", "button up", "button down", "work shirt",
        "everyday shirt", "classic shirt", "plain shirt", "simple shirt", "standard shirt",
        "fitted shirt", "regular shirt", "basic shirt", "office shirt", "collar shirt"],
    
    7: ["sneaker", "sneakers", "trainer", "running shoe", "athletic shoe",
        "sport shoe", "casual sneaker", "tennis shoe", "gym shoe", "walking shoe",
        "everyday sneaker", "comfortable sneaker", "trainers", "sports sneaker", "active shoe",
        "rubber shoe", "lace up sneaker", "low top sneaker", "basic sneaker", "street shoe"],
    
    8: ["bag", "handbag", "purse", "shoulder bag", "tote bag",
        "carry bag", "everyday bag", "casual bag", "hand bag", "fashion bag",
        "women's bag", "shopping bag", "simple bag", "basic bag", "travel bag",
        "carryall", "satchel", "daily bag", "practical bag", "accessory bag"],
    
    9: ["ankle boot", "boot", "short boot", "bootie", "ankle bootie",
        "low boot", "casual boot", "everyday boot", "simple boot", "leather boot",
        "walking boot", "comfortable boot", "fitted boot", "standard boot", "basic boot",
        "ankle high boot", "short ankle boot", "classic boot", "urban boot", "stylish boot"]
}

# %% ../nbs/00_data.ipynb 31
import random
assert set([len(i) for i in captions.values()]) == {len(captions[0])}
rand_lbl = lambda : random.randint(0, len(captions[0])-1)

# %% ../nbs/00_data.ipynb 34
class MNISTDataset(Dataset):
    def __init__(self, tokenizer, fn_path:Path, split='train', captions=captions):
        self.tokenizer = tokenizer
        self.captions = captions
        self.ds = datasets.FashionMNIST(fn_path, train=split == 'train', transform=transforms.ToTensor())

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        im, lbl = self.ds[idx]
        caption = self.captions[lbl][rand_lbl()] + '\n'         # Add '\n' here showing eos for the caption data
        tokens = torch.tensor(self.tokenizer.encode(caption))

        # Create input and target sequences
        input_tokens = tokens[:-1]
        target_tokens = tokens

        return im, input_tokens, target_tokens

# %% ../nbs/00_data.ipynb 40
from torch.nn.utils.rnn import pad_sequence

def collate_fn_multimodal(batch):
    """Collate function to handle variable length sequences"""
    images, input_seqs, target_seqs = zip(*batch)
    
    # Stack images
    images = torch.stack(images)
    
    # Pad input and target sequences
    input_seqs_padded = pad_sequence(input_seqs, batch_first=True, padding_value=0)
    target_seqs_padded = pad_sequence(target_seqs, batch_first=True, padding_value=0)
    
    return images, input_seqs_padded, target_seqs_padded

# Updated dataloader function
def get_mnist_caption_dl(tokenizer, fn_path, batch_size=64):
    """
    Creates train and validation dataloaders for Fashion MNIST with captions.
    Returns dict with 'train' and 'valid' DataLoaders.
    """
    train_ds = MNISTDataset(tokenizer, fn_path, split='train')
    valid_ds = MNISTDataset(tokenizer, fn_path, split='valid')

    return {
        'train': DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_multimodal),
        'valid': DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate_fn_multimodal)
    }
