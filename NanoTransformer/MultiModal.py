# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_MultiModal.ipynb.

# %% auto 0
__all__ = ['text_dl', 'vision_dl', 'multiConfig', 'train_iters', 'valid_iters', 'find_lbl', 'Embedding', 'MultiModal',
           'MultiModalConfig', 'TrainBatchIter', 'ValidBatchIter', 'pred', 'cal_loss', 'calculate_text_loss',
           'calculate_vision_loss', 'total_loss', 'generate_caption', 'generate_text']

# %% ../nbs/03_MultiModal.ipynb 4
from .data import *
from .GPTText2Text import *
from .ImageEncoder import *

# %% ../nbs/03_MultiModal.ipynb 5
#text dataloders
text_dl = get_text_dl()
text_dl

# %% ../nbs/03_MultiModal.ipynb 7
from fastcore.all import *
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader
import torch

# %% ../nbs/03_MultiModal.ipynb 11
vision_dl = get_mnist_caption_dl(tokenizer, path, 512)
len(vision_dl['train']), len(vision_dl['valid'])

# %% ../nbs/03_MultiModal.ipynb 19
import torch
from torch import nn
class Embedding(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()
        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1

        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)
        self.pos_embed =  nn.Embedding(config.seq_len, config.embedding_dim)

    def forward(self, x, start_idx=0):           #bs * seq_len
        return self.embed(x) + self.pos_embed(self.pos_ids[start_idx:start_idx+x.size(1)])     #bs * seq_len * embedding_dim


# %% ../nbs/03_MultiModal.ipynb 20
class MultiModal(nn.Module):
    def __init__(self):
        super().__init__()

        self.vis_encoder = classifier[0]
        self.proj_layer = nn.Linear(visConfig.head_op_dim, gptConfig.embedding_dim)

        self.embed = Embedding(gptConfig)
        self.blocks = nn.ModuleList(
            [
                nn.Sequential(MultiHeadAttention(gptConfig), FFN(gptConfig))
                for _ in range(gptConfig.n_layers)
            ])
        self.layer_norm = nn.LayerNorm(gptConfig.embedding_dim)
        self.lm_head = nn.Linear(gptConfig.embedding_dim, gptConfig.vocab_size)

        for param in self.vis_encoder.parameters():       #freezing the vision encoder
            param.requires_grad = False

    def forward(self, text_idx, image=None):
        if image is not None:
            # Ensure image has the correct dtype before passing to the encoder
            image = image.to(self.proj_layer.weight.dtype)                     # ensure the image input has the correct data type
            img_emb = self.proj_layer(self.vis_encoder(image)).unsqueeze(1)    # (bs, 1, 128)
            img_emb = img_emb + self.embed.pos_embed(self.embed.pos_ids[0:1])  # fetch embeddings at the 0th idx
            text_emb = self.embed(text_idx, start_idx=1)                       # positions start at 1
            x = torch.cat([img_emb, text_emb], dim=1)
        else:
            x = self.embed(text_idx)

        for block in self.blocks:
            x = block(x)
        return self.lm_head(self.layer_norm(x))

# %% ../nbs/03_MultiModal.ipynb 22
from dataclasses import dataclass

@dataclass
class MultiModalConfig:
    bs = 256

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

    num_steps_per_epoch = min(len(text_dl['train']), len(dls['train']))
    epochs = 20

multiConfig = MultiModalConfig()

# %% ../nbs/03_MultiModal.ipynb 25
class TrainBatchIter:
    def __init__(self):
        self.reset()

    def reset(self):
        self.text_dl = iter(text_dl['train'])
        self.vision_dl = iter(vision_dl['train'])
        self.idx = -1
        self.text_exhausted = False
        self.vision_exhausted = False

    def __iter__(self):
        return self

    def __next__(self):
        # If both exhausted, stop
        if self.text_exhausted and self.vision_exhausted:
            raise StopIteration

        self.idx += 1
        use_text = (self.idx % 2 == 0)

        # Adjust if chosen iterator is exhausted
        if use_text and self.text_exhausted:
            use_text = False
        elif not use_text and self.vision_exhausted:
            use_text = True

        # Try to get batch
        try:
            if use_text:
                x, y = next(self.text_dl)
                return x, y, None
            else:
                images, x, y = next(self.vision_dl)
                return x, y, images
        except StopIteration:
            # Mark as exhausted and try the other
            if use_text:
                self.text_exhausted = True
                try:
                    images, x, y = next(self.vision_dl)
                    return x, y, images
                except StopIteration:
                    self.vision_exhausted = True
                    raise
            else:
                self.vision_exhausted = True
                try:
                    x, y = next(self.text_dl)
                    return x, y, None
                except StopIteration:
                    self.text_exhausted = True
                    raise

# %% ../nbs/03_MultiModal.ipynb 26
class ValidBatchIter:
    def __init__(self):
        self.reset()

    def reset(self):
        # Reset the iters
        self.text_dl = iter(text_dl['valid'])
        self.vision_dl = iter(vision_dl['valid'])
        self.text_exhausted = False
        self.vision_exhausted = False

    def __iter__(self):
        return self

    def __next__(self):
        # If both exhausted, stop
        if not self.text_exhausted:
            #iter through the text data first
            #once exausted do not raise stop iter instead iter through vision data
            try:
                x, y = next(self.text_dl)
                return x, y, None
            except StopIteration:
                self.text_exhausted = True

        if self.text_exhausted:
          try:
            # fetch vision data
            images, x, y = next(self.vision_dl)
            return x, y, images

          except StopIteration:
            self.vision_exhausted = True
            raise StopIteration

# %% ../nbs/03_MultiModal.ipynb 28
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

# %% ../nbs/03_MultiModal.ipynb 29
train_iters = TrainBatchIter()
valid_iters = ValidBatchIter()

# %% ../nbs/03_MultiModal.ipynb 30
def pred(model, text_input, ims):
  """prediction for given text and image"""
  return model(text_input, ims)

# %% ../nbs/03_MultiModal.ipynb 32
def cal_loss(logits, text_target):
  """calculate the loss for pred and target"""
  return loss_func(logits.reshape(-1, gptConfig.vocab_size), text_target.reshape(-1))

# %% ../nbs/03_MultiModal.ipynb 34
def calculate_text_loss(model, data_dl):
    total_loss = 0
    num_batches = 0

    for x, y in data_dl:
        x, y = x.to(multiConfig.device), y.to(multiConfig.device)

        with torch.no_grad():
          logits = pred(model, x, None)
          loss = cal_loss(logits, y)

        total_loss += loss.item()
        num_batches += 1

    return total_loss / num_batches

# %% ../nbs/03_MultiModal.ipynb 36
def calculate_vision_loss(model, data_dl):
    total_loss = 0
    num_batches = 0

    for ims, x, y in data_dl:
        ims = ims.to(multiConfig.device)
        x, y = x.to(multiConfig.device), y.to(multiConfig.device)

        with torch.no_grad():
          logits = pred(model, x, ims)
          loss = cal_loss(logits, y)

        total_loss += loss.item()
        num_batches += 1

    return total_loss / num_batches


# %% ../nbs/03_MultiModal.ipynb 38
def total_loss(model):
  text_loss = (calculate_text_loss(model, text_dl['train']) + calculate_text_loss(model, text_dl['valid'])) / 2
  vision_loss = (calculate_vision_loss(model, vision_dl['train']) + calculate_vision_loss(model, vision_dl['valid'])) / 2
  print(f"Text Loss: {text_loss:.4f} | Vision Loss: {vision_loss:.4f}")

# %% ../nbs/03_MultiModal.ipynb 44
@torch.no_grad()
def generate_caption(model, image, max_len=30):
    model.eval()
    image = image.unsqueeze(0).to(multiConfig.device).to(multiConfig.dtype)  # Add batch dimension and move to device with correct dtype

    generated = []
    text_idx = torch.empty((1, 0), dtype=torch.long, device=multiConfig.device)  # Empty text

    for _ in range(max_len):
        logits = model(text_idx, image)
        next_token = logits[:, -1, :].argmax(dim=-1)

        # Check for stop token '\n'
        if tokenizer.decode([next_token.item()]) == '\n':
            break

        generated.append(next_token.item())
        text_idx = torch.cat([text_idx, next_token.unsqueeze(0)], dim=1)

    return tokenizer.decode(generated)

# %% ../nbs/03_MultiModal.ipynb 46
find_lbl = lambda lable: [key for key, values in captions.items() if lable in values]

# %% ../nbs/03_MultiModal.ipynb 51
@torch.no_grad()
def generate_text(model, prompt, max_new_tokens=100, temperature=1.0):
    """
    prompt: string to start generation
    max_new_tokens: how many tokens to generate
    temperature: higher = more random, lower = more deterministic
    """
    model.eval()
    tokens = tokenizer.encode(prompt)
    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim
    tokens = tokens.to(multiConfig.device)
    for _ in range(max_new_tokens):
        # Crop to last seq_len tokens if needed
        context = tokens if tokens.size(1) <= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]

        # Get predictions
        with torch.no_grad(), torch.autocast(device_type=multiConfig.device, dtype=torch.bfloat16):
          logits = model(context)
        logits = logits[:, -1, :] / temperature  # Focus on last token

        # Sample next token
        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Append to sequence
        tokens = torch.cat([tokens, next_token], dim=1)

    return tokenizer.decode(tokens.squeeze().tolist())
