# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_GPTText2Text.ipynb.

# %% auto 0
__all__ = ['gptConfig', 'dls', 'model', 'loss_func', 'optimizer', 'GPTConfig', 'Embedding', 'AttentionHead', 'MultiHeadAttention',
           'FFN', 'GPTModel']

# %% ../nbs/01_GPTText2Text.ipynb 6
from .data import *

# %% ../nbs/01_GPTText2Text.ipynb 10
from dataclasses import dataclass
import torch

@dataclass
class GPTConfig:
    bs = 256
    seq_len = 128                # context length
    embedding_dim = 128          # dim of the embedding layer
    n_layers = 4                # no of decoder block stack on top of each other
    n_heads = 8                 # no of heads in a single decoder block
    vocab_size = len(tokenizer.vocab)
    dropout = 0.1

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

    epochs = 200
gptConfig = GPTConfig()

# %% ../nbs/01_GPTText2Text.ipynb 14
dls = get_text_dl(bs=gptConfig.bs, seq_len=gptConfig.seq_len)
dls

# %% ../nbs/01_GPTText2Text.ipynb 20
import torch
from torch import nn
class Embedding(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()
        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1

        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)
        self.pos_embed =  nn.Embedding(config.seq_len, config.embedding_dim)

    def forward(self, x):           #bs * seq_len
        return self.embed(x) + self.pos_embed(self.pos_ids[:x.size(1)])     #bs * seq_len * embedding_dim

# %% ../nbs/01_GPTText2Text.ipynb 31
class AttentionHead(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        assert config.embedding_dim % config.n_heads == 0
        self.head_dim = config.embedding_dim // config.n_heads

        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)
        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)
        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)


        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))
        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask

        self.dropout = nn.Dropout(p = config.dropout)

    def forward(self, x): #bs * seq_len * embedding_dim

        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)        #bs * seq_len * head_dim

        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len

        attn += self.mask[:x.shape[1], :x.shape[1]]

        attn = torch.softmax(attn, dim=-1)

        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim

# %% ../nbs/01_GPTText2Text.ipynb 34
class MultiHeadAttention(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()
        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer

        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])
        self.dropout = nn.Dropout(p=config.dropout)
        self.linear_MHA = nn.Linear(config.embedding_dim, config.embedding_dim)
        self.layer_norm = nn.LayerNorm(config.embedding_dim)

    def forward(self, x): #bs * seq_len * embedding_dim
        head = torch.cat([head(x) for head in self.heads], dim=-1) #bs * seq_len * embedding_dim
        head = self.dropout(self.linear(head))                     #bs * seq_len * embedding_dim
        return self.layer_norm(head + x)

# %% ../nbs/01_GPTText2Text.ipynb 37
class FFN(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        self.dropout = nn.Dropout(p=config.dropout)
        self.linear1 = nn.Linear(config.embedding_dim, 4 * config.embedding_dim)
        self.linear2 = nn.Linear(4 *config.embedding_dim, config.embedding_dim)
        self.layer_norm = nn.LayerNorm(config.embedding_dim)
        self.gelu = nn.GELU(approximate='tanh')

    def forward(self, x): #bs * seq_len * embedding_dim
        pred = self.linear2(self.gelu(self.linear1(x)))
        return self.layer_norm(self.dropout(pred) + x)

# %% ../nbs/01_GPTText2Text.ipynb 41
class GPTModel(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        self.embed = Embedding(config)
        self.blocks = nn.ModuleList(
            [
                nn.Sequential(MultiHeadAttention(config), FFN(config))
                for _ in range(config.n_layers)
            ])
        self.layer_norm = nn.LayerNorm(config.embedding_dim)
        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)

    def forward(self, x):
        x = self.embed(x)
        for block in self.blocks:
            x = block(x)
        return self.lm_head(self.layer_norm(x))

model = GPTModel(gptConfig)

# %% ../nbs/01_GPTText2Text.ipynb 45
loss_func = nn.CrossEntropyLoss()

# %% ../nbs/01_GPTText2Text.ipynb 50
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

model = GPTModel(gptConfig).to(gptConfig.device)
optimizer = AdamW(model.parameters(), lr=gptConfig.lr)
