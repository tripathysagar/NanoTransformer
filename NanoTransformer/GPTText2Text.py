"""Various components of GPT"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_GPTText2Text.ipynb.

# %% auto 0
__all__ = ['config', 'dls', 'model', 'loss_func', 'optimizer', 'epochs', 'GPTConfig', 'Embedding', 'AttentionHead',
           'MultiHeadAttention', 'FFN', 'GPTModel', 'generate']

# %% ../nbs/01_GPTText2Text.ipynb 6
from .data import *

# %% ../nbs/01_GPTText2Text.ipynb 10
from dataclasses import dataclass
import torch

@dataclass
class GPTConfig:
    bs = 256
    seq_len = 128                # context length
    embedding_dim = 128          # dim of the embedding layer
    n_layers = 4                # no of decoder block stack on top of each other
    n_heads = 8                 # no of heads in a single decoder block
    vocab_size = len(tokenizer.vocab)
    dropout = 0.1

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16

    lr = 1e-3
    max_grad_norm = 1.0

config = GPTConfig()

# %% ../nbs/01_GPTText2Text.ipynb 14
dls = get_text_dl(bs=config.bs, seq_len=config.seq_len)
dls

# %% ../nbs/01_GPTText2Text.ipynb 20
import torch
from torch import nn
class Embedding(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()
        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1

        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)
        self.pos_embed =  nn.Embedding(config.seq_len, config.embedding_dim)

    def forward(self, x):           #bs * seq_len
        return self.embed(x) + self.pos_embed(self.pos_ids[:x.size(1)])     #bs * seq_len * embedding_dim


# %% ../nbs/01_GPTText2Text.ipynb 31
class AttentionHead(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        assert config.embedding_dim % config.n_heads == 0
        self.head_dim = config.embedding_dim // config.n_heads

        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)
        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)
        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)


        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))
        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask

        self.dropout = nn.Dropout(p = config.dropout)

    def forward(self, x): #bs * seq_len * embedding_dim

        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)        #bs * seq_len * head_dim

        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len

        attn += self.mask[:x.shape[1], :x.shape[1]]

        attn = torch.softmax(attn, dim=-1)

        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim

# %% ../nbs/01_GPTText2Text.ipynb 34
class MultiHeadAttention(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()
        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer

        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])
        self.dropout = nn.Dropout(p=config.dropout)
        self.linear = nn.Linear(config.embedding_dim, config.embedding_dim)
        self.layer_norm = nn.LayerNorm(config.embedding_dim)

    def forward(self, x): #bs * seq_len * embedding_dim
        head = torch.cat([head(x) for head in self.heads], dim=-1) #bs * seq_len * embedding_dim
        head = self.dropout(self.linear(head))                     #bs * seq_len * embedding_dim
        return self.layer_norm(head + x)

# %% ../nbs/01_GPTText2Text.ipynb 37
class FFN(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        self.dropout = nn.Dropout(p=config.dropout)
        self.linear1 = nn.Linear(config.embedding_dim, 4 * config.embedding_dim)
        self.linear2 = nn.Linear(4 *config.embedding_dim, config.embedding_dim)
        self.layer_norm = nn.LayerNorm(config.embedding_dim)
        self.gelu = nn.GELU(approximate='tanh')

    def forward(self, x): #bs * seq_len * embedding_dim
        pred = self.linear2(self.gelu(self.linear1(x)))
        return self.layer_norm(self.dropout(pred) + x)

# %% ../nbs/01_GPTText2Text.ipynb 41
class GPTModel(nn.Module):
    def __init__(self, config:GPTConfig):
        super().__init__()

        self.embed = Embedding(config)
        self.blocks = nn.ModuleList(
            [
                nn.Sequential(MultiHeadAttention(config), FFN(config))
                for _ in range(config.n_layers)
            ])
        self.layer_norm = nn.LayerNorm(config.embedding_dim)
        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)

    def forward(self, x):
        x = self.embed(x)
        for block in self.blocks:
            x = block(x)
        return self.lm_head(self.layer_norm(x))

model = GPTModel(config)

# %% ../nbs/01_GPTText2Text.ipynb 45
loss_func = nn.CrossEntropyLoss()

# %% ../nbs/01_GPTText2Text.ipynb 50
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

model = GPTModel(config).to(config.device)
optimizer = AdamW(model.parameters(), lr=config.lr)

# %% ../nbs/01_GPTText2Text.ipynb 52
epochs = 100

for epoch in range(epochs):
    model.train()
    train_loss = 0

    for x, y in dls['train']:
        x, y = x.to(config.device), y.to(config.device)
        optimizer.zero_grad()

        with torch.autocast(device_type=config.device, dtype=config.dtype):
          logits = model(x)
          loss = loss_func(logits.view(-1, config.vocab_size), y.view(-1))

        loss.backward()

        clip_grad_norm_(model.parameters(), config.max_grad_norm) # to clip gradients

        optimizer.step()

        train_loss += loss.item()

    model.eval()
    val_loss = 0
    with torch.no_grad(), torch.autocast(device_type=config.device, dtype=config.dtype):
        for x, y in dls['valid']:
            x, y = x.to(config.device), y.to(config.device)

            logits = model(x)
            loss = loss_func(logits.view(-1, config.vocab_size), y.view(-1))
            val_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(dls['train']):.4f} Validation Loss: {val_loss/len(dls['valid']):.4f}")

# %% ../nbs/01_GPTText2Text.ipynb 54
@torch.no_grad()
def generate(prompt, max_new_tokens=100, temperature=1.0):
    """
    prompt: string to start generation
    max_new_tokens: how many tokens to generate
    temperature: higher = more random, lower = more deterministic
    """
    model.eval()
    tokens = tokenizer.encode(prompt)
    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim
    tokens = tokens.to('cuda')
    for _ in range(max_new_tokens):
        # Crop to last seq_len tokens if needed
        context = tokens if tokens.size(1) <= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]

        # Get predictions
        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):
          logits = model(context)
        logits = logits[:, -1, :] / temperature  # Focus on last token

        # Sample next token
        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Append to sequence
        tokens = torch.cat([tokens, next_token], dim=1)

    return tokenizer.decode(tokens.squeeze().tolist())
print(generate("To be or not to be"))
