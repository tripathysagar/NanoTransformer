{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377fc727",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c397f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "from fastcore.all import *\n",
    "path = Path('../static')\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a688921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ce9f0",
   "metadata": {},
   "source": [
    "## Text Data Pipeline:\n",
    "\n",
    "1. Downloaded Shakespeare text data, from [Karpathy's nanogpt](https://github.com/karpathy/build-nanogpt) \n",
    "1. Built a character-level Tokenizer (65 unique chars)\n",
    "1. Created GPTDataset for non-overlapping sequence chunks\n",
    "1. Made get_text_dl() function returning train/valid dataloaders with 90/10 split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a770a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../static/input.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export \n",
    "urlsave(\"https://raw.githubusercontent.com/karpathy/build-nanogpt/refs/heads/master/input.txt\", path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from typing import List\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Maps each char to unique index. It have some key attributes i.e.\n",
    "    1. **voacb**: where it maped to all the char present text field\n",
    "    1. **encode**: to encode given string to list of tokens\n",
    "    1. **decode**: to decode given tokens to str\n",
    "    1. **c2i** and **i2c**: helper function to convert char to tokens and tokens to char respectively\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.setup_vocab()\n",
    "\n",
    "    def setup_vocab(self):\n",
    "        with open(path/'input.txt', 'r') as file:\n",
    "            self.txt = file.read()\n",
    "\n",
    "        self.vocab = sorted(list(set(list(self.txt))))\n",
    "        #print(f\"After reading file got the vocab of shape : {len(self.vocab)}\")\n",
    "\n",
    "    def c2i(self, ch:str) -> int:\n",
    "        \"\"\"\n",
    "        returns index of char ch from vocab\n",
    "        \"\"\"\n",
    "        return self.vocab.index(ch)\n",
    "\n",
    "    def i2c(self, idx:int) -> str:\n",
    "        \"\"\"\n",
    "        returns char from vocab given index\n",
    "        \"\"\"\n",
    "        return self.vocab[idx]\n",
    "\n",
    "    def encode(self, inp:str) -> List[int]:\n",
    "        \"\"\"\n",
    "        returns the encoded string\n",
    "        \"\"\"\n",
    "        return [self.c2i(i) for i in inp]\n",
    "\n",
    "    def decode(self, inp:List[int]) -> str:\n",
    "        \"\"\"\n",
    "        returns the string represntation of the\n",
    "        \"\"\"\n",
    "        return ''.join([self.i2c(i) for i in inp])\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ae350",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'abc'\n",
    "assert tokenizer.decode(tokenizer.encode(s))  == s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b6d63",
   "metadata": {},
   "source": [
    "The tokenizer is loss less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b3d0a",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "The Dataset should be of **non-overlapping chunks**:\n",
    "1. `__init__` - store the encoded text and config (seq_len, etc.)\n",
    "1. `__len__`: return `len(encoded_text) // seq_len - 1` (divide, not subtract)\n",
    "1. `__getitem__`: use `idx * seq_len` as the starting position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#|hide\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, seq_len:int):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "        self.encoded_text = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) // self.seq_len -1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.encoded_text[idx * self.seq_len : (idx + 1) * self.seq_len]\n",
    "        op = self.encoded_text[idx * self.seq_len + 1 : (idx + 1) * self.seq_len + 1]\n",
    "        return torch.tensor(inp), torch.tensor(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b82d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_text_dl(bs:int=64, seq_len:int=128):\n",
    "    split_idx = int(len(tokenizer.txt) * 0.9)                         #split text with 9:\n",
    "    train_dataset = GPTDataset(tokenizer.txt[:split_idx], seq_len )\n",
    "    val_dataset = GPTDataset(tokenizer.txt[split_idx:], seq_len)\n",
    "\n",
    "    return {\n",
    "        'train': DataLoader(train_dataset, batch_size=bs, shuffle=True),\n",
    "        'valid': DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_text_dl()\n",
    "for x, y in dl['train']:\n",
    "    break\n",
    "assert x.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a04bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([52, 53, 59, 57, 50, 63, 11,  1, 40, 43]),\n",
       " tensor([53, 59, 57, 50, 63, 11,  1, 40, 43, 50]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :10], y[0,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf786d",
   "metadata": {},
   "source": [
    "## Vision Fashion MNIST\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Define transforms** - Convert images to tensors, normalize if needed\n",
    "\n",
    "1. **Load Fashion MNIST dataset** - Use `torchvision.datasets.FashionMNIST()` for train and test splits\n",
    "\n",
    "1. **Create DataLoaders** - Wrap datasets with DataLoader, set batch_size and shuffle=True for training\n",
    "\n",
    "1. **Verify** - Check one batch to confirm shape (bs, 1, 28, 28) and labels (0-9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429404d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../static\n",
       "    Split: Train"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from torchvision import datasets, transforms\n",
    "datasets.FashionMNIST(path, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('../static/input.txt'),Path('../static/FashionMNIST')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(im, label=None, n=1, figsize=(2,2)):\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    if isinstance(im, torch.Tensor):\n",
    "        if im.shape[0] == 1:\n",
    "            im = im.squeeze(0)  # Remove channel dim for grayscale\n",
    "        elif im.shape[0] == 3:\n",
    "            im = im.permute(1, 2, 0)  # Change to (H, W, C)\n",
    "        im = im.numpy()\n",
    "\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(im, cmap='gray')\n",
    "        if label: ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ccb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = datasets.FashionMNIST(path, train=True, transform=transforms.ToTensor())\n",
    "valid_ds = datasets.FashionMNIST(path, transform=transforms.ToTensor())\n",
    "im, lbl = train_ds[0]\n",
    "type(im), lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689688c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dl:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb60c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(*valid_ds[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761230c7",
   "metadata": {},
   "source": [
    "A helper function for retruning fashion mnist classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_vision_classifier_dl(bs:int=64):\n",
    "    im_path = path/'FashionMNIST'\n",
    "\n",
    "    train_ds = datasets.FashionMNIST(im_path, train=True, transform=transforms.ToTensor())\n",
    "    valid_ds = datasets.FashionMNIST(im_path, transform=transforms.ToTensor())\n",
    "\n",
    "    return {\n",
    "        'train': DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        'valid': DataLoader(valid_ds, batch_size=bs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82520a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = get_vision_classifier_dl()\n",
    "for x, y in dl['valid']:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(x[-1], y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b72333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
