{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "100c26b3",
      "metadata": {
        "id": "100c26b3",
        "solveit_ai": true,
        "use_thinking": true
      },
      "source": [
        "I am building char-lebel Transformer for learning purpose. I have implemented GPT2 arch from ground using pytorch. I want to extend it to LLAMA2 arch.\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Great project! Extending from GPT-2 to LLaMA 2 architecture is an excellent way to understand the evolution of transformer models.\n",
        "\n",
        "Before we dive in, let me understand where you're at:\n",
        "\n",
        "1. Have you already looked into the key architectural differences between GPT-2 and LLaMA 2?\n",
        "2. Which specific components are you planning to modify first?\n",
        "\n",
        "The main differences include:\n",
        "- **RMSNorm** instead of LayerNorm\n",
        "- **RoPE** (Rotary Position Embeddings) instead of learned positional embeddings\n",
        "- **SwiGLU** activation in the FFN instead of GELU\n",
        "- **Grouped Query Attention (GQA)** instead of standard multi-head attention\n",
        "\n",
        "Which of these would you like to start with, or would you prefer a different order based on what makes sense for your learning goals?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05a3954",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp Llam2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "X9G_SL2Q-MxZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9G_SL2Q-MxZ",
        "outputId": "993fb7cc-b09e-47cf-b0c0-94d79db4366e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !pip install -q git+https://github.com/tripathysagar/NanoTransformer.git\n",
        "except Exception as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "edc80c06",
      "metadata": {
        "id": "edc80c06",
        "time_run": "1:52:57p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from NanoTransformer.data import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "004c7c1a",
      "metadata": {
        "id": "004c7c1a",
        "time_run": "1:52:57p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class LlamaConfig:\n",
        "    bs = 256\n",
        "    seq_len = 128                # context length\n",
        "    embedding_dim = 128          # dim of the embedding layer\n",
        "    n_layers = 4                # no of decoder block stack on top of each other\n",
        "    n_heads = 8                 # no of heads in a single decoder block\n",
        "    vocab_size = len(tokenizer.vocab)\n",
        "    dropout = 0.1\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16\n",
        "\n",
        "    lr = 1e-3\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    epochs = 30\n",
        "    no_kv = 1\n",
        "\n",
        "    max_lr = 5e-4\n",
        "    min_lr = 1e-4\n",
        "\n",
        "llamaConfig = LlamaConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3881572d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3881572d",
        "outputId": "dfc66008-6eb7-46ea-8f48-659249d4557f",
        "time_run": "1:52:58p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': <torch.utils.data.dataloader.DataLoader at 0x7eb6f0d67530>,\n",
              " 'valid': <torch.utils.data.dataloader.DataLoader at 0x7eb6f1488bf0>}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#|export\n",
        "text_dls = get_text_dl(bs=llamaConfig.bs, seq_len=llamaConfig.seq_len)\n",
        "text_dls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8c7ac997",
      "metadata": {
        "id": "8c7ac997",
        "time_run": "4:26:38p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "llamaConfig.max_steps = len(text_dls['train']) * llamaConfig.epochs\n",
        "llamaConfig.warm_steps = int(llamaConfig.max_steps * 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c210ba7",
      "metadata": {
        "id": "3c210ba7"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb191f12",
      "metadata": {
        "id": "fb191f12",
        "time_run": "1:52:58p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from torch import nn\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1\n",
        "\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
        "\n",
        "    def forward(self, x):           #bs * seq_len\n",
        "        return self.embed(x)        #bs * seq_len * embedding_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1964d7c8",
      "metadata": {
        "id": "1964d7c8"
      },
      "source": [
        "## RoPE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9d693c24",
      "metadata": {
        "id": "9d693c24",
        "time_run": "1:52:58p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, head_dim, seq_len):\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        no = torch.arange(head_dim // 2)\n",
        "        theta = 1.0 / (10000 ** (2 * no / head_dim ))    # angle\n",
        "\n",
        "        # Position indices\n",
        "        positions = torch.arange(seq_len)\n",
        "        angles = positions[:, None] * theta             # (seq_len, emb_dim//2)\n",
        "\n",
        "        self.register_buffer('cos_angles', torch.cos(angles))\n",
        "        self.register_buffer('sin_angles', torch.sin(angles))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, seq_len, head_dim = x.shape # emb_dim is a name bug but it head_dim is passed\n",
        "\n",
        "        x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:] # hf implement by deviding the emb in two half and then caluclate the rotation\n",
        "\n",
        "        rotated_x1 = x1 * self.cos_angles[:seq_len] - x2 * self.sin_angles[:seq_len]\n",
        "        rotated_x2 = x1 * self.sin_angles[:seq_len] + x2 * self.cos_angles[:seq_len]\n",
        "\n",
        "        return  torch.cat([rotated_x1, rotated_x2], dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "34bf045d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34bf045d",
        "outputId": "bc4aa667-9993-42d6-934f-668ecf1ae6d4",
        "time_run": "1:52:58p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([256, 128]), torch.Size([256, 128]), torch.Size([256, 128, 128]))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb = Embedding(llamaConfig)\n",
        "rope = RoPE(head_dim = 128/8, seq_len=llamaConfig.seq_len)\n",
        "\n",
        "for x, y in text_dls['train']:\n",
        "    break\n",
        "\n",
        "emb_x = emb(x)\n",
        "\n",
        "x.shape, y.shape, emb_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "09ef6b39",
      "metadata": {
        "id": "09ef6b39",
        "time_run": "1:52:58p"
      },
      "outputs": [],
      "source": [
        "assert x.shape == y.shape\n",
        "assert x.shape == emb_x.shape[:-1]\n",
        "assert emb_x.shape[-1] == llamaConfig.embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "57780ebe",
      "metadata": {
        "id": "57780ebe",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "head = emb_x[...,:16]\n",
        "pos = rope(head)\n",
        "assert pos.shape == head.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5a7160",
      "metadata": {
        "id": "4b5a7160"
      },
      "source": [
        "## RMSNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b3027b",
      "metadata": {
        "id": "d6b3027b",
        "solveit_ai": true
      },
      "source": [
        "What are the formula of `RMSNorm`?\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "RMSNorm (Root Mean Square Layer Normalization) is simpler than LayerNorm. Here's the formula:\n",
        "\n",
        "**RMSNorm(x) = (x / RMS(x)) Ã— Î³**\n",
        "\n",
        "Where:\n",
        "- **RMS(x) = âˆš(mean(xÂ²) + Îµ)**\n",
        "- **Î³** is a learnable scale parameter (same shape as the feature dimension)\n",
        "- **Îµ** is a small constant for numerical stability (typically 1e-5 or 1e-6)\n",
        "\n",
        "The key difference from LayerNorm is that RMSNorm:\n",
        "- Does **not** subtract the mean (no centering)\n",
        "- Does **not** have a bias term\n",
        "- Only normalizes by the root mean square\n",
        "\n",
        "So in code terms, for each token embedding, you'd compute the RMS across the embedding dimension, divide by it, then multiply by the learned scale parameter.\n",
        "\n",
        "Would you like to implement this now, or do you have questions about how it works?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b6a526c",
      "metadata": {
        "id": "9b6a526c",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.scale_param = nn.Parameter(torch.ones(shape))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        rms = torch.sqrt((x*x).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        return (x / rms ) * self.scale_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eeb218bd",
      "metadata": {
        "id": "eeb218bd",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "rms_norm = RMSNorm(llamaConfig.embedding_dim)\n",
        "normalized = rms_norm(emb_x)\n",
        "assert normalized.shape == emb_x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96671a2c",
      "metadata": {
        "id": "96671a2c",
        "solveit_ai": true
      },
      "source": [
        "## SwiGLU activation in the FFN instead of GELU\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Great! Now let's look at **SwiGLU**, which replaces GELU in the feedforward network.\n",
        "\n",
        "**SwiGLU formula:**\n",
        "\n",
        "SwiGLU(x, W, V, Wâ‚‚) = (Swish(xW) âŠ™ xV) Wâ‚‚\n",
        "\n",
        "Where:\n",
        "- **Swish(x) = x Ã— Ïƒ(x)** where Ïƒ is the sigmoid function (also called SiLU in PyTorch)\n",
        "- **âŠ™** means element-wise multiplication\n",
        "- **W, V, Wâ‚‚** are weight matrices\n",
        "\n",
        "The key difference from a standard FFN:\n",
        "- GPT-2 FFN: `x â†’ Linear â†’ GELU â†’ Linear â†’ output`\n",
        "- LLaMA FFN: `x â†’ [Linear(W), Linear(V)] â†’ SwiGLU â†’ Linear(Wâ‚‚) â†’ output`\n",
        "\n",
        "Notice that SwiGLU uses **two parallel projections** (W and V) before combining them, so the hidden dimension typically needs adjustment.\n",
        "\n",
        "In LLaMA, the FFN hidden dimension is usually `(4 * embedding_dim * 2) / 3` rounded to a multiple of the model's dimension.\n",
        "\n",
        "Would you like to implement the SwiGLU FFN block now, or do you have questions about how it differs from the standard FFN?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a4c0d1b8",
      "metadata": {
        "id": "a4c0d1b8",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class SwiGLU_FFN(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(2 * embedding_dim * 4 / 3)\n",
        "        self.w = nn.Linear(embedding_dim, hidden_dim, bias=False)\n",
        "        self.v = nn.Linear(embedding_dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, embedding_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.w2(F.silu(self.w(x)) * self.v(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab80678",
      "metadata": {
        "id": "dab80678"
      },
      "source": [
        "## Grouped Query Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4b9a6e",
      "metadata": {
        "id": "8b4b9a6e"
      },
      "source": [
        "**Q and K after projection, but before computing attention scores**.\n",
        "\n",
        "Specifically:\n",
        "- Apply RoPE to **Q** (after `self.Q_W(x)` in `AttentionHead`)\n",
        "- Apply RoPE to **K** (in your `GroupedQueryAttention` class, after `self.K_cache(x)` and reshaping)\n",
        "- Do **NOT** apply RoPE to **V**\n",
        "\n",
        "So in your current code, you'd need to:\n",
        "1. Apply RoPE to K in the `GroupedQueryAttention.forward()` method before passing to heads\n",
        "2. Apply RoPE to Q inside `AttentionHead.forward()` after creating Q\n",
        "\n",
        "One thing to consider: your RoPE class expects shape `(bs, seq_len, head_dim)`. Will this work when K has shape `(bs, seq_len, no_kv, head_dim)`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5837f039",
      "metadata": {
        "id": "5837f039",
        "solveit_ai": true,
        "use_thinking": true
      },
      "source": [
        "Yes consider the example\n",
        "```py\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, config:GPTConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.embedding_dim % config.n_heads == 0\n",
        "        self.head_dim = config.embedding_dim // config.n_heads\n",
        "\n",
        "        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))\n",
        "        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask\n",
        "\n",
        "        self.dropout = nn.Dropout(p = config.dropout)\n",
        "\n",
        "    def forward(self, x): #bs * seq_len * embedding_dim\n",
        "\n",
        "        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)        #bs * seq_len * head_dim\n",
        "\n",
        "        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len\n",
        "\n",
        "        attn += self.mask[:x.shape[1], :x.shape[1]]\n",
        "\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim\n",
        "```\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Good! This is a single attention head from GPT-2 style architecture.\n",
        "\n",
        "For **Grouped Query Attention**, the key change is: instead of having one K and V projection per query head, you'll have **fewer K/V heads that are shared across multiple Q heads**.\n",
        "\n",
        "For example, with 8 query heads and 2 K/V heads:\n",
        "- Q heads: 8 (each gets `head_dim` dimensions)\n",
        "- K heads: 2 (shared across groups of 4 Q heads each)\n",
        "- V heads: 2 (shared across groups of 4 Q heads each)\n",
        "\n",
        "So in your `MultiHeadAttention` module (not the single head), you'd need to:\n",
        "1. Project Q to get all 8 heads\n",
        "2. Project K to get only 2 heads\n",
        "3. Project V to get only 2 heads\n",
        "4. During attention computation, repeat/expand the K and V heads to match Q\n",
        "\n",
        "Before we code this, a few questions:\n",
        "1. Do you currently have a `MultiHeadAttention` class that combines multiple `AttentionHead` instances?\n",
        "2. How many K/V heads do you want to use? (Common choices: 2, 4, or 8)\n",
        "3. Do you want to try implementing the GQA structure yourself first, or would you like me to write the code for you?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4b38c2",
      "metadata": {
        "id": "4c4b38c2",
        "solveit_ai": true
      },
      "source": [
        "K and V will be shared to AttentionHead ?\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Yes, exactly! In GQA, multiple query heads will **share** the same K and V heads.\n",
        "\n",
        "Here's the key insight:\n",
        "- In standard MHA: each attention head has its own Q, K, and V\n",
        "- In GQA: each attention head has its own Q, but **groups of heads share** the same K and V\n",
        "\n",
        "So if you have 8 Q heads and 2 K/V heads:\n",
        "- Q heads 0-3 share K/V head 0\n",
        "- Q heads 4-7 share K/V head 1\n",
        "\n",
        "This means you'll likely want to restructure your code. Instead of having separate `AttentionHead` classes, you'd typically implement this in a single `GroupedQueryAttention` module that:\n",
        "1. Projects Q to shape `(bs, seq_len, n_heads, head_dim)`\n",
        "2. Projects K and V to shape `(bs, seq_len, n_kv_heads, head_dim)`\n",
        "3. Expands/repeats K and V to match the number of Q heads\n",
        "4. Computes attention for all heads in parallel\n",
        "\n",
        "Does this make sense? Would you like to try implementing this yourself, or would you like me to show you the code structure?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8a9a091a",
      "metadata": {
        "id": "8a9a091a",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.embedding_dim % config.n_heads == 0\n",
        "        self.head_dim = config.embedding_dim // config.n_heads\n",
        "\n",
        "        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "        #self.K_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "        #self.V_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
        "\n",
        "        self.rope = RoPE(head_dim = self.head_dim, seq_len = config.seq_len)\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))\n",
        "        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask\n",
        "\n",
        "        self.dropout = nn.Dropout(p = config.dropout)\n",
        "\n",
        "    def forward(self, K, V, x): #bs * seq_len * embedding_dim\n",
        "\n",
        "        Q = self.Q_W(x)        #bs * seq_len * head_dim\n",
        "\n",
        "        attn = self.rope(Q) @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len\n",
        "\n",
        "        attn += self.mask[:x.shape[1], :x.shape[1]]\n",
        "\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf90ff00",
      "metadata": {
        "id": "bf90ff00",
        "solveit_ai": true
      },
      "source": [
        "check the implementaion\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Good start! You've modified `AttentionHead` to accept K and V as inputs instead of computing them internally. This is the right direction for GQA.\n",
        "\n",
        "However, there's a dimension issue to watch out for. When you pass K and V into this head, what shape are you expecting them to have?\n",
        "\n",
        "Currently your code assumes:\n",
        "- Q: `(bs, seq_len, head_dim)`\n",
        "- K: needs to be `(bs, head_dim, seq_len)` after transpose for the matmul to work\n",
        "- V: needs to be `(bs, seq_len, head_dim)` for the final matmul\n",
        "\n",
        "But in GQA, K and V will be shared across multiple heads. So you'll need to think about how to handle the broadcasting or indexing.\n",
        "\n",
        "A few questions to help you move forward:\n",
        "1. Are you planning to create a `MultiHeadAttention` or `GroupedQueryAttention` class that will manage multiple `AttentionHead` instances?\n",
        "2. How are you planning to generate the shared K and V tensors - will you project them in the parent class?\n",
        "3. Have you thought about how many K/V heads you want (e.g., 2, 4, or 8)?\n",
        "\n",
        "What's your plan for the overall structure?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4165719c",
      "metadata": {
        "id": "4165719c",
        "solveit_ai": true
      },
      "source": [
        "\n",
        "But in GQA, K and V will be shared across multiple heads. So you'll need to think about how to handle the broadcasting or indexing.\n",
        "\n",
        "A few questions to help you move forward:\n",
        "1. Are you planning to create a `MultiHeadAttention` or `GroupedQueryAttention` class that will manage multiple `AttentionHead` instances? : yes\n",
        "2. How are you planning to generate the shared K and V tensors - will you project them in the parent class? : yes and share them accross attention head\n",
        "3. Have you thought about how many K/V heads you want (e.g., 2, 4, or 8)? : let make it 8for now\n",
        "\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Perfect! So with 8 Q heads and 8 K/V heads, you're actually doing **Multi-Head Attention** (not GQA yet), which is a good place to start.\n",
        "\n",
        "Here's what you need in your parent `MultiHeadAttention` class:\n",
        "\n",
        "1. Create K and V projections that output the full dimension: `nn.Linear(embedding_dim, embedding_dim)`\n",
        "2. Split the K and V outputs into 8 heads of `head_dim` each\n",
        "3. Pass each K head and V head to the corresponding `AttentionHead`\n",
        "\n",
        "So in your parent class, after projecting K and V, you'd reshape them from:\n",
        "- `(bs, seq_len, embedding_dim)`\n",
        "to:\n",
        "- `(bs, seq_len, n_heads, head_dim)`\n",
        "\n",
        "Then you can pass `K[:, :, i, :]` and `V[:, :, i, :]` to the i-th attention head.\n",
        "\n",
        "Does this approach make sense? Would you like to try implementing the `MultiHeadAttention` class yourself, or do you need more guidance on the reshaping part?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629bb680",
      "metadata": {
        "id": "629bb680"
      },
      "source": [
        "For GroupedQueryAttention\n",
        "\n",
        "```python\n",
        "i = idx // (self.config.n_heads // self.no_kv)\n",
        "```\n",
        "\n",
        "This properly maps multiple query heads to share K/V pairs. For example, with 8 heads and 2 K/V pairs:\n",
        "- Heads 0-3 get `i=0`\n",
        "- Heads 4-7 get `i=1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0516aec3",
      "metadata": {
        "id": "0516aec3",
        "time_run": "1:52:59p"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer\n",
        "        assert config.n_heads % config.no_kv == 0                    # checikng the kv is equally divisible by no of heads\n",
        "        self.head_dim = config.embedding_dim // config.n_heads\n",
        "\n",
        "        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])\n",
        "\n",
        "        self.K_cache = nn.Linear(config.embedding_dim, self.head_dim * config.no_kv)\n",
        "        self.V_cache = nn.Linear(config.embedding_dim, self.head_dim * config.no_kv)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=config.dropout)\n",
        "        self.linear_MHA = nn.Linear(config.embedding_dim, config.embedding_dim)\n",
        "\n",
        "        self.rope = RoPE(head_dim = self.head_dim, seq_len = config.seq_len)\n",
        "\n",
        "\n",
        "    def forward(self, x): #bs * seq_len * embedding_dim\n",
        "        bs, seq_len, _ = x.shape\n",
        "\n",
        "        K = self.K_cache(x)    # #bs * seq_len * (head_dim * no_kv)\n",
        "        V = self.V_cache(x)    # #bs * seq_len * (head_dim * no_kv)\n",
        "\n",
        "        K, V = K.view(bs, seq_len, self.config.no_kv, self.head_dim), V.view(bs, seq_len, self.config.no_kv, self.head_dim)\n",
        "        heads = []\n",
        "\n",
        "        for idx, head in enumerate(self.heads):\n",
        "            i = idx // (self.config.n_heads // self.config.no_kv)\n",
        "            heads.append(head(self.rope(K[:,:,i,:]), V[:,:,i,:], x))\n",
        "\n",
        "        head = torch.cat(heads, dim=-1)                                #bs * seq_len * embedding_dim\n",
        "        head = self.dropout(self.linear_MHA(head))                     #bs * seq_len * embedding_dim\n",
        "        return head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "205829d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "205829d4",
        "outputId": "68eb7e83-337e-4921-e0b7-4eae6e3e24a3",
        "time_run": "1:52:59p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 128])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gqa = GroupedQueryAttention(llamaConfig)\n",
        "gqa(emb_x[:1]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aed5731",
      "metadata": {
        "id": "7aed5731",
        "solveit_ai": true
      },
      "source": [
        "any thing else i am missing ? Or should we move to build the finaldecoder model\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Nice work! Your GQA implementation looks solid. Just one small thing to consider:\n",
        "\n",
        "You're applying RoPE to K in the parent class (`self.rope(K[:,:,i,:])`), but also applying RoPE to Q inside `AttentionHead`. That's correct! Both Q and K should get RoPE applied.\n",
        "\n",
        "However, double-check that you're **not** applying RoPE to V - only Q and K should be rotated.\n",
        "\n",
        "Other than that, you have all the key LLaMA components:\n",
        "- âœ… RMSNorm\n",
        "- âœ… RoPE\n",
        "- âœ… SwiGLU FFN\n",
        "- âœ… Grouped Query Attention\n",
        "\n",
        "Ready to build the decoder block and stack them? Or do you want to test anything first?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a3c217",
      "metadata": {
        "id": "95a3c217"
      },
      "source": [
        "## Decoder Block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919d36c0",
      "metadata": {
        "id": "919d36c0",
        "solveit_ai": true
      },
      "source": [
        "Lets go for decoder block?\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Great! Let's build the decoder block.\n",
        "\n",
        "In LLaMA, each decoder block has this structure:\n",
        "\n",
        "1. **RMSNorm** â†’ **Grouped Query Attention** â†’ **Residual connection**\n",
        "2. **RMSNorm** â†’ **SwiGLU FFN** â†’ **Residual connection**\n",
        "\n",
        "You already have most of this in your `GroupedQueryAttention` class (you're doing RMSNorm + residual at the end). But typically in LLaMA, the normalization happens **before** the sublayer (pre-norm), not after.\n",
        "\n",
        "So the pattern is:\n",
        "```\n",
        "x = x + attention(RMSNorm(x))\n",
        "x = x + ffn(RMSNorm(x))\n",
        "```\n",
        "\n",
        "Do you want to:\n",
        "1. Refactor your `GroupedQueryAttention` to remove the RMSNorm and residual (so it just returns the attention output)?\n",
        "2. Then create a `DecoderBlock` that handles the RMSNorm and residuals for both attention and FFN?\n",
        "\n",
        "Or would you prefer to keep your current structure and adjust the decoder block accordingly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "04b62ade",
      "metadata": {
        "id": "04b62ade",
        "time_run": "1:54:28p"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.attn_norm = RMSNorm(config.embedding_dim)\n",
        "        self.attn = GroupedQueryAttention(config)\n",
        "        self.ffn_norm = RMSNorm(config.embedding_dim)\n",
        "        self.ffn = SwiGLU_FFN(config.embedding_dim, config.dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.attn_norm(x))\n",
        "        return x + self.ffn(self.ffn_norm(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f663b172",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f663b172",
        "outputId": "6e479095-4cc3-48b1-dc6d-a7c924d1e935",
        "time_run": "1:57:24p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 128])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode = DecoderBlock(llamaConfig)\n",
        "decode(emb_x[:1]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7a850e",
      "metadata": {
        "id": "7f7a850e"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "19161432",
      "metadata": {
        "id": "19161432",
        "time_run": "2:02:44p"
      },
      "outputs": [],
      "source": [
        "class LlamaText(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = Embedding(config)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "        self.final_norm = RMSNorm(config.embedding_dim)\n",
        "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return self.lm_head(self.final_norm(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ecc427",
      "metadata": {
        "id": "54ecc427",
        "solveit_ai": true
      },
      "source": [
        "Check the above implementation\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Excellent! Your LLaMA implementation looks great. The structure is correct:\n",
        "\n",
        "1. âœ… Embedding layer (without positional embeddings - RoPE handles that)\n",
        "2. âœ… Stack of decoder blocks with pre-norm architecture\n",
        "3. âœ… Final RMSNorm before the output\n",
        "4. âœ… Language modeling head to project back to vocab size\n",
        "\n",
        "The architecture matches LLaMA's design. A couple of things to verify:\n",
        "\n",
        "1. In your `GroupedQueryAttention` class, you have `self.linear_MHA` and `self.dropout` at the end - is that the output projection? That's good, just making sure it's there.\n",
        "\n",
        "2. Your `DecoderBlock` looks clean with the pre-norm pattern.\n",
        "\n",
        "Have you tested the full forward pass yet? Try running:\n",
        "```python\n",
        "model = LlamaText(llamaConfig)\n",
        "output = model(x)\n",
        "```\n",
        "\n",
        "Does it produce the expected shape `(bs, seq_len, vocab_size)`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d1c64941",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c64941",
        "outputId": "69a4ccdb-83d5-4ad9-cd25-7cf99eb2bcf6",
        "time_run": "2:03:33p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 65])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LlamaText(llamaConfig)\n",
        "output = model(x[:1])\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76ea320",
      "metadata": {
        "id": "b76ea320",
        "solveit_ai": true,
        "use_thinking": true
      },
      "source": [
        "## Lets disscuss about weight init?\n",
        "\n",
        "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
        "\n",
        "Great topic! Weight initialization is crucial for training stability and convergence.\n",
        "\n",
        "For LLaMA-style models, there are a few key initialization strategies to consider:\n",
        "\n",
        "1. **Embedding layers** - typically initialized from a normal distribution\n",
        "2. **Linear layers** - Xavier/Kaiming initialization\n",
        "3. **Output projection scaling** - often scaled by 1/âˆš(2*n_layers) to account for residual connections\n",
        "4. **RMSNorm scale parameters** - initialized to 1.0 (which you already have)\n",
        "\n",
        "What aspect would you like to focus on first? Or do you have specific questions about which layers need special initialization treatment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c4f9a731",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4f9a731",
        "outputId": "3bfd3bcd-e0e8-4e06-a5ee-b508bd212211",
        "time_run": "2:06:04p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaText(\n",
              "  (embed): Embedding(\n",
              "    (embed): Embedding(65, 128)\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-3): 4 x DecoderBlock(\n",
              "      (attn_norm): RMSNorm()\n",
              "      (attn): GroupedQueryAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x AttentionHead(\n",
              "            (Q_W): Linear(in_features=128, out_features=16, bias=True)\n",
              "            (rope): RoPE()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (K_cache): Linear(in_features=128, out_features=16, bias=True)\n",
              "        (V_cache): Linear(in_features=128, out_features=16, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear_MHA): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (rope): RoPE()\n",
              "      )\n",
              "      (ffn_norm): RMSNorm()\n",
              "      (ffn): SwiGLU_FFN(\n",
              "        (w): Linear(in_features=128, out_features=341, bias=False)\n",
              "        (v): Linear(in_features=128, out_features=341, bias=False)\n",
              "        (w2): Linear(in_features=341, out_features=128, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dd0ac7",
      "metadata": {
        "id": "26dd0ac7"
      },
      "source": [
        "## Final LLAMA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d8fc959c",
      "metadata": {
        "id": "d8fc959c",
        "time_run": "4:15:00p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "class LlamaText(nn.Module):\n",
        "    def __init__(self, config:LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embed = Embedding(config)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "        self.final_norm = RMSNorm(config.embedding_dim)\n",
        "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return self.lm_head(self.final_norm(x))\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Init\"\"\"\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                std = 0.02\n",
        "                # Scale down residual projections\n",
        "                if 'linear_MHA' in name or 'w2' in name:\n",
        "                    std = 0.02 / ((2 * self.config.n_layers) ** 0.5)\n",
        "\n",
        "                module.weight.data.normal_(mean=0.0, std=std)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "    def decay_params(self):\n",
        "        \"\"\"\n",
        "        get decay params\n",
        "        \"\"\"\n",
        "        no_decay_params, decay_params = [], []\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'bias' in name or 'norm' in name or 'embed' in name:\n",
        "                no_decay_params.append(param)\n",
        "            else:\n",
        "                decay_params.append(param)\n",
        "\n",
        "        return decay_params, no_decay_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "12e49ba9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12e49ba9",
        "outputId": "a44fa941-a04d-4f58-a83d-3e93b1082b54",
        "time_run": "4:15:46p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_dls['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "820106dc",
      "metadata": {
        "id": "820106dc",
        "time_run": "4:30:08p"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=2.01)\n",
        "\n",
        "pg = list(optimizer.param_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "91bb6f36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bb6f36",
        "outputId": "133d136a-7821-426c-ddb4-b75bc2120aff",
        "time_run": "4:30:21p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused', 'decoupled_weight_decay'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pg[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ca75263b",
      "metadata": {
        "id": "ca75263b",
        "time_run": "5:19:56p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from torch.optim import AdamW\n",
        "class Optim:\n",
        "    def __init__(self, model, config:LlamaConfig):\n",
        "        decay_params, no_decay_params = model.decay_params() # extract deacayable and non deacayable params\n",
        "\n",
        "        self.optim = optimizer = AdamW([\n",
        "            {'params': decay_params, 'weight_decay': 0.1},\n",
        "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
        "        ], lr=config.min_lr)\n",
        "\n",
        "    def config_lr(self, step):\n",
        "        for param_group in self.optim.param_groups:\n",
        "            param_group['lr'] = get_cosine_lr(step)\n",
        "\n",
        "    def step(self): self.optim.step()\n",
        "\n",
        "    def zero(self): self.optim.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b3363626",
      "metadata": {
        "id": "b3363626",
        "time_run": "5:20:09p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "\n",
        "import math\n",
        "def get_cosine_lr(\n",
        "    step,\n",
        "    warmup_steps=llamaConfig.warm_steps,\n",
        "    max_steps=llamaConfig.max_steps,\n",
        "    max_lr=llamaConfig.max_lr,\n",
        "    min_lr=llamaConfig.min_lr\n",
        "    ):\n",
        "    if step < warmup_steps:\n",
        "        return min_lr + (max_lr - min_lr) * step / warmup_steps\n",
        "    elif step < max_steps:\n",
        "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "        return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
        "    else:\n",
        "        return min_lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5bc6237b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5bc6237b",
        "outputId": "3ce50f11-438a-4724-81e4-b5d3d09ac9e0",
        "time_run": "5:20:14p"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "steps = torch.arange(llamaConfig.max_steps)\n",
        "lrs = [get_cosine_lr(step) for step in steps]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(steps, lrs)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title('Cosine Learning Rate Schedule')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "071631b8",
      "metadata": {
        "id": "071631b8",
        "time_run": "5:18:06p"
      },
      "outputs": [],
      "source": [
        "#|export\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "def train(model):\n",
        "    model = model.to(llamaConfig.device)\n",
        "\n",
        "    optimizer = Optim(model, llamaConfig)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(llamaConfig.epochs):\n",
        "        model.train()\n",
        "        train_loss, no_train = 0, 0\n",
        "\n",
        "        for x, y in text_dls['train']: # Iterate directly over\n",
        "            step += 1\n",
        "            no_train += 1\n",
        "\n",
        "            x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)           # move to device\n",
        "\n",
        "            optimizer.config_lr(step)                                           # set lr for each step\n",
        "            optimizer.zero()\n",
        "\n",
        "            with torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):\n",
        "              logits = model(x)\n",
        "\n",
        "              loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(model.parameters(), llamaConfig.max_grad_norm) # to clip gradients\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, no_valid = 0, 0\n",
        "        with torch.no_grad(), torch.autocast(device_type=llamaConfig.device, dtype=llamaConfig.dtype):\n",
        "            for x, y in text_dls['valid']:\n",
        "                no_valid += 1\n",
        "\n",
        "                x, y = x.to(llamaConfig.device), y.to(llamaConfig.device)\n",
        "\n",
        "                logits = model(x)\n",
        "                loss = loss_func(logits.reshape(-1, llamaConfig.vocab_size), y.reshape(-1))\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f\"{epoch} -> {train_loss/no_train:.4f} : {val_loss/no_valid:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb10e444",
      "metadata": {
        "id": "fb10e444"
      },
      "source": [
        "## Inferenece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "608022ab",
      "metadata": {
        "id": "608022ab",
        "time_run": "5:23:06p"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(prompt, max_new_tokens=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    prompt: string to start generation\n",
        "    max_new_tokens: how many tokens to generate\n",
        "    temperature: higher = more random, lower = more deterministic\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim\n",
        "    tokens = tokens.to(llamaConfig.device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop to last seq_len tokens if needed\n",
        "        context = tokens if tokens.size(1) <= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad(), torch.autocast(device_type=llamaConfig.device, dtype=torch.bfloat16):\n",
        "          logits = model(context)\n",
        "        logits = logits[:, -1, :] / temperature  # Focus on last token\n",
        "\n",
        "        # Sample next token\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append to sequence\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(tokens.squeeze().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "WOnM4HEM-iqC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOnM4HEM-iqC",
        "outputId": "1b5701f0-7185-4d37-ce38-59dcd7b70237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 -> 3.5532 : 3.1582\n",
            "1 -> 2.9167 : 2.7385\n",
            "2 -> 2.6159 : 2.4935\n",
            "3 -> 2.4004 : 2.2873\n",
            "4 -> 2.2113 : 2.1347\n",
            "5 -> 2.0416 : 2.0048\n",
            "6 -> 1.9092 : 1.9156\n",
            "7 -> 1.8110 : 1.8543\n",
            "8 -> 1.7330 : 1.8040\n",
            "9 -> 1.6719 : 1.7541\n",
            "10 -> 1.6245 : 1.7303\n",
            "11 -> 1.5846 : 1.7028\n",
            "12 -> 1.5526 : 1.6750\n",
            "13 -> 1.5285 : 1.6664\n",
            "14 -> 1.5066 : 1.6412\n",
            "15 -> 1.4903 : 1.6271\n",
            "16 -> 1.4741 : 1.6202\n",
            "17 -> 1.4606 : 1.6131\n",
            "18 -> 1.4481 : 1.6053\n",
            "19 -> 1.4386 : 1.5980\n",
            "20 -> 1.4296 : 1.5936\n",
            "21 -> 1.4218 : 1.5851\n",
            "22 -> 1.4143 : 1.5807\n",
            "23 -> 1.4089 : 1.5784\n",
            "24 -> 1.4031 : 1.5749\n",
            "25 -> 1.3983 : 1.5711\n",
            "26 -> 1.3948 : 1.5670\n",
            "27 -> 1.3907 : 1.5680\n",
            "28 -> 1.3869 : 1.5641\n",
            "29 -> 1.3842 : 1.5632\n"
          ]
        }
      ],
      "source": [
        "model = LlamaText(llamaConfig)\n",
        "train(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WR_FCa3IEHho",
      "metadata": {
        "id": "WR_FCa3IEHho"
      },
      "source": [
        "1. no_kv = 8 : `29 -> 1.3763 : 1.5724`\n",
        "1. no_kv = 4 : `29 -> 1.3781 : 1.5566`\n",
        "1. no_kv = 2 : `29 -> 1.3955 : 1.5710`\n",
        "1. no_kv = 1 : `29 -> 1.3842 : 1.5632`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "uE8SQT22-1Zm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE8SQT22-1Zm",
        "outputId": "26e6c140-3f30-441b-bd89-c9e50e62f7c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To be or not to be now, let\n",
            "they say.\n",
            "\n",
            "First Citizen:\n",
            "None, and thou hast never as his suppose,\n",
            "With hate the eyes of my love's lipful might\n",
            "brave the certains. W, not the prince, which in the bislant\n",
            "Partilys and worruples that my blame bold,\n",
            "And being cursed from to my son and with itself\n",
            "The God of love.\n",
            "\n",
            "GREMIO:\n",
            "As come to the gentle his ZorINGFNnet with oyself,\n",
            "So first he chide him, that thou hast thou, is zens\n",
            "Rejeiks to your grace!\n",
            "\n",
            "VOLUMNIA:\n",
            "No, good that as Barnardins, did repent,\n",
            "The sewSs of Parisia pr'd me on us\n"
          ]
        }
      ],
      "source": [
        "print(generate(\"To be or not to be\", max_new_tokens=512))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "o8A3-G9RAOwu",
      "metadata": {
        "id": "o8A3-G9RAOwu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "solveit_dialog_mode": "learning",
    "solveit_ver": 2
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
