{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4f8517a3",
   "metadata": {},
   "source": [
    "---\n",
    "description: Various components of GPT\n",
    "output-file: gpttext2text.html\n",
    "title: GPTText2Text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp GPTText2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-kT_SWO_H_tT",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Large language models like GPT have revolutionized natural language processing, but their architectures can seem complex and mysterious. In this tutorial, we demystify the GPT decoder architecture by building one from scratch using PyTorch.\n",
    "\n",
    "This implementation is inspired by Andrej Karpathy's excellent \"Let's build GPT\" [tutorial](https://youtu.be/kCc8FmEb1nY), which provides a fantastic walkthrough of transformer fundamentals.\n",
    "\n",
    "We'll implement a character-level language model trained on Shakespeare's complete works, constructing every component step-by-step: tokenization, embeddings, multi-head attention with causal masking, feed-forward networks, and the complete decoder stack. By working with a smaller model (33K parameters), we can understand the fundamentals without requiring extensive computational resources.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BbfeL-pC5Oop",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TbSrq_ga4yk-",
   "metadata": {},
   "source": [
    "### Download input\n",
    "The text is taken from [Karpathy's nanogpt](https://github.com/karpathy/build-nanogpt) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vOoIje_vDGw4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install -q git+https://github.com/tripathysagar/NanoTransformer.git\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RewLm16WDhoD",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from NanoTransformer.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71265aa",
   "metadata": {},
   "source": [
    "### Read text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db76471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.vocab)\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-s8ts9Qk67Ev",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jl63fDuP6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    bs = 256\n",
    "    seq_len = 128                # context length\n",
    "    embedding_dim = 128          # dim of the embedding layer\n",
    "    n_layers = 4                # no of decoder block stack on top of each other\n",
    "    n_heads = 8                 # no of heads in a single decoder block\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    dropout = 0.1\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16\n",
    "\n",
    "    lr = 1e-3\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    epochs = 75\n",
    "gptConfig = GPTConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1d814",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Maps each char to unique index. It have some key attributes i.e.\n",
    "1. **voacb**: where it maped to all the char present text field\n",
    "1. **encode**: to encode given string to list of tokens\n",
    "1. **decode**: to decode given tokens to str\n",
    "1. **c2i** and **i2c**: helper function to convert char to tokens and tokens to char respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'abc'\n",
    "assert tokenizer.decode(tokenizer.encode(s))  == s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C-5xjXPB5CxI",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "The Dataset should be of **non-overlapping chunks**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1NbqC3C5CxJ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader>,\n",
       " 'valid': <torch.utils.data.dataloader.DataLoader>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "dls = get_text_dl(bs=gptConfig.bs, seq_len=gptConfig.seq_len)\n",
    "dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2YpNbjVFa0z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sequence lengths correct\n"
     ]
    }
   ],
   "source": [
    "for x, y in dls['train']:\n",
    "  break\n",
    "assert x.shape == y.shape\n",
    "assert x.shape[0] == gptConfig.bs\n",
    "assert x.shape[1] == gptConfig.seq_len\n",
    "print(\"✓ Sequence lengths correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3vvBF2aBF_8x",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train:len(v)=31', 'valid:len(v)=4']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{k}:{len(v)=}\" for k, v in dls.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4RTtIKTI8Sko",
   "metadata": {},
   "source": [
    "The dataloders have around 62 batch of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c8638",
   "metadata": {},
   "source": [
    "## Attention Mechanisim\n",
    "\n",
    "![pasted_image_be04a721-d6fa-4a79-a961-2e851b81545a.png](attachment:be04a721-d6fa-4a79-a961-2e851b81545a)\n",
    "Single Decoder Block contains:\n",
    "\n",
    "Masked Multi-Head Attention - prevents looking at future tokens (causal mask)\n",
    "Add & Norm - residual connection + layer normalization\n",
    "Feed Forward - two linear layers with activation (usually GELU)\n",
    "Add & Norm - another residual connection + layer norm\n",
    "Full Model structure:\n",
    "\n",
    "Input Embedding - converts token indices to vectors\n",
    "Positional Encoding - adds position information (learned or fixed)\n",
    "N× Decoder Blocks - stacked blocks (GPT-2 small uses 12)\n",
    "Linear layer - projects to vocabulary size\n",
    "Softmax - converts to probabilities\n",
    "Key difference from full transformer: GPT decoder only has the right side - no encoder, no cross-attention (the \"Add & Norm + Multi-Head Attention\" in the middle). Just masked self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aae91e",
   "metadata": {},
   "source": [
    "### Input Embedding Layer\n",
    "It consists of two type of embedding.\n",
    "1. token embedding: for the all the tokens in the vocab\n",
    "1. Postion embedding: for encode the postions encoding to learn one after another\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "from torch import nn\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1\n",
    "\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.pos_embed =  nn.Embedding(config.seq_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, x):           #bs * seq_len\n",
    "        return self.embed(x) + self.pos_embed(self.pos_ids[:x.size(1)])     #bs * seq_len * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(config=gptConfig)\n",
    "x = torch.tensor([1, 2, 3])[None,:] #input sequence\n",
    "pred_embed = embed(x)\n",
    "\n",
    "assert x.shape == pred_embed.shape[:2]\n",
    "assert pred_embed.shape[-1] == gptConfig.embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5439c03",
   "metadata": {},
   "source": [
    "### Single attention head\n",
    "Map all the tokens in seq_len(context window) where we keep track of how each token affects other token.\n",
    "\n",
    "It is the heart of of **Transformer** architecture\n",
    "1. A input is mapped to query `Q`, key `K` and value `V` linear parameters\n",
    "1. **Causal mask**- This is crucial for GPT! It prevents the model from \"looking ahead\" at future tokens. You mask out positions that come after the current position.\n",
    "1. **Softmax**- after the scaled dot prod of Q and K, then applay softmax to get attention weight\n",
    "1. **Dropout**- added addition regularization\n",
    "\n",
    "The final formula is :\n",
    "    ```\n",
    "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k) + mask) @ V\n",
    "    ```\n",
    "\n",
    "Where:\n",
    "- `d_k` is the dim of the key(head_dim)\n",
    "- The mask ensures causality (no peeking at future tokens)\n",
    "\n",
    "**Causal mask**\n",
    "- During training, the model sees the entire sequence at once\n",
    "- Without masking, it could \"cheat\" by looking at future tokens\n",
    "- The mask ensures each position only attends to previous positions (autoregressive)\n",
    "\n",
    "  Example for seq_len=3:\n",
    "  ```\n",
    "  Position 0: can only see position 0\n",
    "  Position 1: can see positions 0, 1\n",
    "  Position 2: can see positions 0, 1, 2\n",
    "  ```\n",
    "\n",
    "- *How it works?*\n",
    "  1. by converting the mask with lower trigular matrix filled with 1\n",
    "  1. substituting the 0 with `-inf`\n",
    "  1. using softmax to zeros out these fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c148d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = gptConfig.embedding_dim // gptConfig.n_heads\n",
    "assert gptConfig.embedding_dim % gptConfig.n_heads == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_W = nn.Linear(gptConfig.embedding_dim, head_dim)\n",
    "K_W = nn.Linear(gptConfig.embedding_dim, head_dim)\n",
    "V_W = nn.Linear(gptConfig.embedding_dim, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02a5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 16]), torch.Size([1, 3, 16]), torch.Size([1, 3, 16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = Q_W(pred_embed)\n",
    "k = K_W(pred_embed)\n",
    "v = V_W(pred_embed)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a00fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = q @ k.transpose(-2, -1) # Shape: (bs, seq_len, seq_len)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964bf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., -inf, -inf],\n",
      "        [1., 1., -inf],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3))\n",
    "print(mask)\n",
    "mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dd90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.6983,   -inf,   -inf],\n",
       "         [2.8262, 0.2329,   -inf],\n",
       "         [1.8241, 3.0780, 5.0162]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = attn + mask\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610747e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000],\n",
       "         [0.9304, 0.0696, 0.0000],\n",
       "         [0.0347, 0.1215, 0.8439]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(attn, dim =-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100317e9",
   "metadata": {},
   "source": [
    "This is the heart of the attention block, which blocks out future tokens. By setting the masked entries to negative infinity `-inf`, the softmax operation turns these values into approximately zero $e^{-\\infty}=0$. This effectively prevents the model from attending to future tokens, ensuring causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.embedding_dim % config.n_heads == 0\n",
    "        self.head_dim = config.embedding_dim // config.n_heads\n",
    "\n",
    "        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))\n",
    "        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask\n",
    "\n",
    "        self.dropout = nn.Dropout(p = config.dropout)\n",
    "\n",
    "    def forward(self, x): #bs * seq_len * embedding_dim\n",
    "\n",
    "        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)        #bs * seq_len * head_dim\n",
    "\n",
    "        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len\n",
    "\n",
    "        attn += self.mask[:x.shape[1], :x.shape[1]]\n",
    "\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f5552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 128])\n",
      "Output shape: torch.Size([2, 5, 16])\n",
      "Expected head_dim: 16\n"
     ]
    }
   ],
   "source": [
    "attn_head = AttentionHead(gptConfig)\n",
    "test_input = torch.randn(2, 5, gptConfig.embedding_dim)  # bs=2, seq_len=5\n",
    "output = attn_head(test_input)\n",
    "\n",
    "assert output.shape == (2, 5, gptConfig.embedding_dim // gptConfig.n_heads)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected head_dim: {gptConfig.embedding_dim // gptConfig.n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675a387",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "In each decoder block multiple attention head stacked together to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b377838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.linear = nn.Linear(config.embedding_dim, config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "\n",
    "    def forward(self, x): #bs * seq_len * embedding_dim\n",
    "        head = torch.cat([head(x) for head in self.heads], dim=-1) #bs * seq_len * embedding_dim\n",
    "        head = self.dropout(self.linear(head))                     #bs * seq_len * embedding_dim\n",
    "        return self.layer_norm(head + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test 1 passed: Shape preserved\n",
      "✓ Test 2 passed: Causality maintained\n",
      "✓ Test 3 passed: Residual connection active\n",
      "✓ Test 4 passed: Different inputs → different outputs\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Shape preservation\n",
    "mha = MultiHeadAttention(gptConfig)\n",
    "test_input = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "output = mha(test_input)\n",
    "assert output.shape == test_input.shape, \"Shape mismatch!\"\n",
    "print(\"✓ Test 1 passed: Shape preserved\")\n",
    "\n",
    "mha.eval()\n",
    "# Test 2: Causality check\n",
    "torch.manual_seed(42)\n",
    "test_input = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "output1 = mha(test_input)\n",
    "\n",
    "# Modify last token\n",
    "test_input_modified = test_input.clone()\n",
    "test_input_modified[:, -1, :] = torch.randn(2, gptConfig.embedding_dim)\n",
    "output2 = mha(test_input_modified)\n",
    "\n",
    "# First tokens should be identical (not affected by future)\n",
    "assert torch.allclose(output1[:, 0, :], output2[:, 0, :], atol=1e-5), \"Causality violated!\"\n",
    "print(\"✓ Test 2 passed: Causality maintained\")\n",
    "\n",
    "# Test 3: Residual connection working\n",
    "torch.manual_seed(42)\n",
    "test_input = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "output = mha(test_input)\n",
    "# Output should be different from input (but related due to residual)\n",
    "assert not torch.allclose(output, test_input), \"Output shouldn't equal input\"\n",
    "print(\"✓ Test 3 passed: Residual connection active\")\n",
    "\n",
    "# Test 4: Different inputs produce different outputs\n",
    "input1 = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "input2 = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "out1 = mha(input1)\n",
    "out2 = mha(input2)\n",
    "assert not torch.allclose(out1, out2), \"Different inputs should produce different outputs\"\n",
    "print(\"✓ Test 4 passed: Different inputs → different outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd6ec7",
   "metadata": {},
   "source": [
    "### Feed Forward Network (FFN)\n",
    "The Feed Forward Network is applied to each position independently after the attention layer.\n",
    "\n",
    "1. **Two linear layers** - expands then contracts the dimensionality\n",
    "   - First layer: `embedding_dim → 4 * embedding_dim` (typical expansion factor is 4)\n",
    "   - Second layer: `4 * embedding_dim → embedding_dim`\n",
    "2. **Activation function** - GELU (Gaussian Error Linear Unit) is commonly used in GPT models, though ReLU also works\n",
    "3. **Dropout** - applied after the second linear layer for regularization\n",
    "4. **Residual connection + Layer Norm** - same pattern as in attention\n",
    "\n",
    "The formula is:\n",
    "`FFN(x) = LayerNorm(Dropout(Linear2(GELU(Linear1(x)))) + x)`\n",
    "\n",
    "\n",
    "This allows the model to process information from the attention layer and make non-linear transformations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.linear1 = nn.Linear(config.embedding_dim, 4 * config.embedding_dim)\n",
    "        self.linear2 = nn.Linear(4 *config.embedding_dim, config.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "\n",
    "    def forward(self, x): #bs * seq_len * embedding_dim\n",
    "        pred = self.linear2(self.gelu(self.linear1(x)))\n",
    "        return self.layer_norm(self.dropout(pred) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Shape preservation\n",
    "ffn = FFN(gptConfig)\n",
    "test_input = torch.randn(2, 5, gptConfig.embedding_dim)\n",
    "output = ffn(test_input)\n",
    "assert output.shape == test_input.shape, \"Shape mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5db8",
   "metadata": {},
   "source": [
    "\n",
    "A single decoder block is straightforward now - it's just combining these two pieces in sequence:\n",
    "\n",
    "```python\n",
    "x → MultiHeadAttention → FFN → output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51554894",
   "metadata": {},
   "source": [
    "## Final model\n",
    "We have implemented the all the components of the attention block, and time has come to wrap up to a single unit DecoderBlock.\n",
    "\n",
    "`\n",
    "x → MultiHeadAttention → FFN → output\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(MultiHeadAttention(config), FFN(config))\n",
    "                for _ in range(config.n_layers)\n",
    "            ])\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.lm_head(self.layer_norm(x))\n",
    "\n",
    "model = GPTModel(gptConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-bf7Rl7rAdL_",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826433"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xR_PuHLwAfDB",
   "metadata": {},
   "source": [
    "Model Architecture Overview:\n",
    "1. Vocabulary size `len(vocab)`: 65 (unique chars)\n",
    "1. Embedding dimension `Embedding.embed`: 128\n",
    "1. Number of layers `MultiHeadAttention` on top of each other: 8\n",
    "1. Number of attention heads `AttentionHead` in a attention head: 8\n",
    "1. Context length `config.seq_len`: 128 tokens\n",
    "1. Total parameters: 33793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30062d7",
   "metadata": {},
   "source": [
    "## Loss function **CrossEntropyLoss**.\n",
    "\n",
    "\n",
    "\n",
    "A key detail: the model outputs shape `(bs, seq_len, vocab_size)`, but CrossEntropyLoss expects `(bs * seq_len, vocab_size)` for input and `(bs * seq_len)` for targets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7da172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041507eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 128]), torch.Size([256, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in dls['train']:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55878fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128, 65])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(x)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd930f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2966, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(logits.view(-1,gptConfig.vocab_size), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988db98",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "model = GPTModel(gptConfig).to(gptConfig.device)\n",
    "optimizer = AdamW(model.parameters(), lr=gptConfig.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LWJ7-lxaBEKi",
   "metadata": {},
   "source": [
    "Please switch the run time to GPU. The training will take some time. Take a break and enjoy a cup of ☕."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cf5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 3.0104 Validation Loss: 2.6568\n",
      "Epoch 2/200, Train Loss: 2.5700 Validation Loss: 2.5209\n",
      "Epoch 3/200, Train Loss: 2.4969 Validation Loss: 2.4792\n",
      "Epoch 4/200, Train Loss: 2.4562 Validation Loss: 2.4387\n",
      "Epoch 5/200, Train Loss: 2.4110 Validation Loss: 2.4001\n",
      "Epoch 6/200, Train Loss: 2.3606 Validation Loss: 2.3436\n",
      "Epoch 7/200, Train Loss: 2.2957 Validation Loss: 2.2682\n",
      "Epoch 8/200, Train Loss: 2.2161 Validation Loss: 2.1847\n",
      "Epoch 9/200, Train Loss: 2.1318 Validation Loss: 2.1201\n",
      "Epoch 10/200, Train Loss: 2.0595 Validation Loss: 2.0650\n",
      "Epoch 11/200, Train Loss: 1.9976 Validation Loss: 2.0251\n",
      "Epoch 12/200, Train Loss: 1.9425 Validation Loss: 1.9926\n",
      "Epoch 13/200, Train Loss: 1.8969 Validation Loss: 1.9581\n",
      "Epoch 14/200, Train Loss: 1.8553 Validation Loss: 1.9319\n",
      "Epoch 15/200, Train Loss: 1.8223 Validation Loss: 1.9125\n",
      "Epoch 16/200, Train Loss: 1.7947 Validation Loss: 1.8919\n",
      "Epoch 17/200, Train Loss: 1.7698 Validation Loss: 1.8735\n",
      "Epoch 18/200, Train Loss: 1.7517 Validation Loss: 1.8561\n",
      "Epoch 19/200, Train Loss: 1.7390 Validation Loss: 1.8505\n",
      "Epoch 20/200, Train Loss: 1.7270 Validation Loss: 1.8449\n",
      "Epoch 21/200, Train Loss: 1.7086 Validation Loss: 1.8354\n",
      "Epoch 22/200, Train Loss: 1.6921 Validation Loss: 1.8144\n",
      "Epoch 23/200, Train Loss: 1.6753 Validation Loss: 1.8110\n",
      "Epoch 24/200, Train Loss: 1.6613 Validation Loss: 1.8010\n",
      "Epoch 25/200, Train Loss: 1.6458 Validation Loss: 1.7916\n",
      "Epoch 26/200, Train Loss: 1.6340 Validation Loss: 1.7771\n",
      "Epoch 27/200, Train Loss: 1.6212 Validation Loss: 1.7723\n",
      "Epoch 28/200, Train Loss: 1.6130 Validation Loss: 1.7699\n",
      "Epoch 29/200, Train Loss: 1.6031 Validation Loss: 1.7519\n",
      "Epoch 30/200, Train Loss: 1.5940 Validation Loss: 1.7577\n",
      "Epoch 31/200, Train Loss: 1.5848 Validation Loss: 1.7465\n",
      "Epoch 32/200, Train Loss: 1.5766 Validation Loss: 1.7429\n",
      "Epoch 33/200, Train Loss: 1.5680 Validation Loss: 1.7387\n",
      "Epoch 34/200, Train Loss: 1.5614 Validation Loss: 1.7402\n",
      "Epoch 35/200, Train Loss: 1.5572 Validation Loss: 1.7312\n",
      "Epoch 36/200, Train Loss: 1.5506 Validation Loss: 1.7286\n",
      "Epoch 37/200, Train Loss: 1.5447 Validation Loss: 1.7267\n",
      "Epoch 38/200, Train Loss: 1.5381 Validation Loss: 1.7150\n",
      "Epoch 39/200, Train Loss: 1.5329 Validation Loss: 1.7100\n",
      "Epoch 40/200, Train Loss: 1.5273 Validation Loss: 1.7099\n",
      "Epoch 41/200, Train Loss: 1.5224 Validation Loss: 1.7036\n",
      "Epoch 42/200, Train Loss: 1.5181 Validation Loss: 1.7099\n",
      "Epoch 43/200, Train Loss: 1.5130 Validation Loss: 1.7080\n",
      "Epoch 44/200, Train Loss: 1.5105 Validation Loss: 1.6971\n",
      "Epoch 45/200, Train Loss: 1.5039 Validation Loss: 1.6959\n",
      "Epoch 46/200, Train Loss: 1.5027 Validation Loss: 1.7037\n",
      "Epoch 47/200, Train Loss: 1.4994 Validation Loss: 1.6933\n",
      "Epoch 48/200, Train Loss: 1.4961 Validation Loss: 1.6919\n",
      "Epoch 49/200, Train Loss: 1.4891 Validation Loss: 1.6880\n",
      "Epoch 50/200, Train Loss: 1.4881 Validation Loss: 1.6829\n",
      "Epoch 51/200, Train Loss: 1.4848 Validation Loss: 1.6789\n",
      "Epoch 52/200, Train Loss: 1.4817 Validation Loss: 1.6836\n",
      "Epoch 53/200, Train Loss: 1.4785 Validation Loss: 1.6813\n",
      "Epoch 54/200, Train Loss: 1.4745 Validation Loss: 1.6833\n",
      "Epoch 55/200, Train Loss: 1.4740 Validation Loss: 1.6824\n",
      "Epoch 56/200, Train Loss: 1.4700 Validation Loss: 1.6781\n",
      "Epoch 57/200, Train Loss: 1.4675 Validation Loss: 1.6785\n",
      "Epoch 58/200, Train Loss: 1.4646 Validation Loss: 1.6781\n",
      "Epoch 59/200, Train Loss: 1.4604 Validation Loss: 1.6732\n",
      "Epoch 60/200, Train Loss: 1.4597 Validation Loss: 1.6657\n",
      "Epoch 61/200, Train Loss: 1.4576 Validation Loss: 1.6748\n",
      "Epoch 62/200, Train Loss: 1.4553 Validation Loss: 1.6729\n",
      "Epoch 63/200, Train Loss: 1.4531 Validation Loss: 1.6648\n",
      "Epoch 64/200, Train Loss: 1.4521 Validation Loss: 1.6720\n",
      "Epoch 65/200, Train Loss: 1.4493 Validation Loss: 1.6774\n",
      "Epoch 66/200, Train Loss: 1.4479 Validation Loss: 1.6639\n",
      "Epoch 67/200, Train Loss: 1.4463 Validation Loss: 1.6639\n",
      "Epoch 68/200, Train Loss: 1.4465 Validation Loss: 1.6668\n",
      "Epoch 69/200, Train Loss: 1.4438 Validation Loss: 1.6639\n",
      "Epoch 70/200, Train Loss: 1.4429 Validation Loss: 1.6629\n",
      "Epoch 71/200, Train Loss: 1.4401 Validation Loss: 1.6611\n",
      "Epoch 72/200, Train Loss: 1.4389 Validation Loss: 1.6581\n",
      "Epoch 73/200, Train Loss: 1.4367 Validation Loss: 1.6641\n",
      "Epoch 74/200, Train Loss: 1.4352 Validation Loss: 1.6633\n",
      "Epoch 75/200, Train Loss: 1.4341 Validation Loss: 1.6565\n",
      "Epoch 76/200, Train Loss: 1.4317 Validation Loss: 1.6597\n",
      "Epoch 77/200, Train Loss: 1.4302 Validation Loss: 1.6637\n",
      "Epoch 78/200, Train Loss: 1.4282 Validation Loss: 1.6525\n",
      "Epoch 79/200, Train Loss: 1.4306 Validation Loss: 1.6604\n",
      "Epoch 80/200, Train Loss: 1.4276 Validation Loss: 1.6496\n",
      "Epoch 81/200, Train Loss: 1.4274 Validation Loss: 1.6604\n",
      "Epoch 82/200, Train Loss: 1.4253 Validation Loss: 1.6564\n",
      "Epoch 83/200, Train Loss: 1.4214 Validation Loss: 1.6554\n",
      "Epoch 84/200, Train Loss: 1.4207 Validation Loss: 1.6471\n",
      "Epoch 85/200, Train Loss: 1.4188 Validation Loss: 1.6610\n",
      "Epoch 86/200, Train Loss: 1.4200 Validation Loss: 1.6532\n",
      "Epoch 87/200, Train Loss: 1.4178 Validation Loss: 1.6599\n",
      "Epoch 88/200, Train Loss: 1.4179 Validation Loss: 1.6590\n",
      "Epoch 89/200, Train Loss: 1.4158 Validation Loss: 1.6569\n",
      "Epoch 90/200, Train Loss: 1.4153 Validation Loss: 1.6499\n",
      "Epoch 91/200, Train Loss: 1.4128 Validation Loss: 1.6515\n",
      "Epoch 92/200, Train Loss: 1.4114 Validation Loss: 1.6562\n",
      "Epoch 93/200, Train Loss: 1.4115 Validation Loss: 1.6548\n",
      "Epoch 94/200, Train Loss: 1.4094 Validation Loss: 1.6516\n",
      "Epoch 95/200, Train Loss: 1.4084 Validation Loss: 1.6503\n",
      "Epoch 96/200, Train Loss: 1.4096 Validation Loss: 1.6543\n",
      "Epoch 97/200, Train Loss: 1.4073 Validation Loss: 1.6466\n",
      "Epoch 98/200, Train Loss: 1.4056 Validation Loss: 1.6579\n",
      "Epoch 99/200, Train Loss: 1.4043 Validation Loss: 1.6531\n",
      "Epoch 100/200, Train Loss: 1.4031 Validation Loss: 1.6460\n",
      "Epoch 101/200, Train Loss: 1.4030 Validation Loss: 1.6490\n",
      "Epoch 102/200, Train Loss: 1.4008 Validation Loss: 1.6571\n",
      "Epoch 103/200, Train Loss: 1.4003 Validation Loss: 1.6649\n",
      "Epoch 104/200, Train Loss: 1.3987 Validation Loss: 1.6564\n",
      "Epoch 105/200, Train Loss: 1.4003 Validation Loss: 1.6562\n",
      "Epoch 106/200, Train Loss: 1.3963 Validation Loss: 1.6495\n",
      "Epoch 107/200, Train Loss: 1.3956 Validation Loss: 1.6469\n",
      "Epoch 108/200, Train Loss: 1.3943 Validation Loss: 1.6482\n",
      "Epoch 109/200, Train Loss: 1.3941 Validation Loss: 1.6536\n",
      "Epoch 110/200, Train Loss: 1.3935 Validation Loss: 1.6558\n",
      "Epoch 111/200, Train Loss: 1.3937 Validation Loss: 1.6519\n",
      "Epoch 112/200, Train Loss: 1.3932 Validation Loss: 1.6509\n",
      "Epoch 113/200, Train Loss: 1.3912 Validation Loss: 1.6608\n",
      "Epoch 114/200, Train Loss: 1.3906 Validation Loss: 1.6587\n",
      "Epoch 115/200, Train Loss: 1.3887 Validation Loss: 1.6466\n",
      "Epoch 116/200, Train Loss: 1.3878 Validation Loss: 1.6488\n",
      "Epoch 117/200, Train Loss: 1.3871 Validation Loss: 1.6546\n",
      "Epoch 118/200, Train Loss: 1.3860 Validation Loss: 1.6483\n",
      "Epoch 119/200, Train Loss: 1.3878 Validation Loss: 1.6542\n",
      "Epoch 120/200, Train Loss: 1.3876 Validation Loss: 1.6538\n",
      "Epoch 121/200, Train Loss: 1.3872 Validation Loss: 1.6488\n",
      "Epoch 122/200, Train Loss: 1.3859 Validation Loss: 1.6493\n",
      "Epoch 123/200, Train Loss: 1.3842 Validation Loss: 1.6521\n",
      "Epoch 124/200, Train Loss: 1.3836 Validation Loss: 1.6477\n",
      "Epoch 125/200, Train Loss: 1.3814 Validation Loss: 1.6448\n",
      "Epoch 126/200, Train Loss: 1.3820 Validation Loss: 1.6489\n",
      "Epoch 127/200, Train Loss: 1.3830 Validation Loss: 1.6487\n",
      "Epoch 128/200, Train Loss: 1.3821 Validation Loss: 1.6484\n",
      "Epoch 129/200, Train Loss: 1.3792 Validation Loss: 1.6466\n",
      "Epoch 130/200, Train Loss: 1.3810 Validation Loss: 1.6517\n",
      "Epoch 131/200, Train Loss: 1.3806 Validation Loss: 1.6518\n",
      "Epoch 132/200, Train Loss: 1.3817 Validation Loss: 1.6489\n",
      "Epoch 133/200, Train Loss: 1.3794 Validation Loss: 1.6454\n",
      "Epoch 134/200, Train Loss: 1.3756 Validation Loss: 1.6557\n",
      "Epoch 135/200, Train Loss: 1.3750 Validation Loss: 1.6505\n",
      "Epoch 136/200, Train Loss: 1.3753 Validation Loss: 1.6450\n",
      "Epoch 137/200, Train Loss: 1.3730 Validation Loss: 1.6433\n",
      "Epoch 138/200, Train Loss: 1.3728 Validation Loss: 1.6430\n",
      "Epoch 139/200, Train Loss: 1.3729 Validation Loss: 1.6466\n",
      "Epoch 140/200, Train Loss: 1.3731 Validation Loss: 1.6560\n",
      "Epoch 141/200, Train Loss: 1.3738 Validation Loss: 1.6514\n",
      "Epoch 142/200, Train Loss: 1.3724 Validation Loss: 1.6475\n",
      "Epoch 143/200, Train Loss: 1.3717 Validation Loss: 1.6519\n",
      "Epoch 144/200, Train Loss: 1.3713 Validation Loss: 1.6449\n",
      "Epoch 145/200, Train Loss: 1.3699 Validation Loss: 1.6464\n",
      "Epoch 146/200, Train Loss: 1.3685 Validation Loss: 1.6519\n",
      "Epoch 147/200, Train Loss: 1.3687 Validation Loss: 1.6445\n",
      "Epoch 148/200, Train Loss: 1.3693 Validation Loss: 1.6525\n",
      "Epoch 149/200, Train Loss: 1.3664 Validation Loss: 1.6363\n",
      "Epoch 150/200, Train Loss: 1.3677 Validation Loss: 1.6452\n",
      "Epoch 151/200, Train Loss: 1.3682 Validation Loss: 1.6532\n",
      "Epoch 152/200, Train Loss: 1.3681 Validation Loss: 1.6565\n",
      "Epoch 153/200, Train Loss: 1.3670 Validation Loss: 1.6476\n",
      "Epoch 154/200, Train Loss: 1.3655 Validation Loss: 1.6400\n",
      "Epoch 155/200, Train Loss: 1.3634 Validation Loss: 1.6465\n",
      "Epoch 156/200, Train Loss: 1.3624 Validation Loss: 1.6506\n",
      "Epoch 157/200, Train Loss: 1.3623 Validation Loss: 1.6428\n",
      "Epoch 158/200, Train Loss: 1.3609 Validation Loss: 1.6471\n",
      "Epoch 159/200, Train Loss: 1.3614 Validation Loss: 1.6472\n",
      "Epoch 160/200, Train Loss: 1.3623 Validation Loss: 1.6572\n",
      "Epoch 161/200, Train Loss: 1.3601 Validation Loss: 1.6507\n",
      "Epoch 162/200, Train Loss: 1.3588 Validation Loss: 1.6534\n",
      "Epoch 163/200, Train Loss: 1.3554 Validation Loss: 1.6497\n",
      "Epoch 164/200, Train Loss: 1.3597 Validation Loss: 1.6447\n",
      "Epoch 165/200, Train Loss: 1.3585 Validation Loss: 1.6471\n",
      "Epoch 166/200, Train Loss: 1.3580 Validation Loss: 1.6395\n",
      "Epoch 167/200, Train Loss: 1.3600 Validation Loss: 1.6452\n",
      "Epoch 168/200, Train Loss: 1.3548 Validation Loss: 1.6382\n",
      "Epoch 169/200, Train Loss: 1.3549 Validation Loss: 1.6508\n",
      "Epoch 170/200, Train Loss: 1.3515 Validation Loss: 1.6511\n",
      "Epoch 171/200, Train Loss: 1.3544 Validation Loss: 1.6433\n",
      "Epoch 172/200, Train Loss: 1.3564 Validation Loss: 1.6511\n",
      "Epoch 173/200, Train Loss: 1.3528 Validation Loss: 1.6390\n",
      "Epoch 174/200, Train Loss: 1.3508 Validation Loss: 1.6440\n",
      "Epoch 175/200, Train Loss: 1.3521 Validation Loss: 1.6435\n",
      "Epoch 176/200, Train Loss: 1.3511 Validation Loss: 1.6429\n",
      "Epoch 177/200, Train Loss: 1.3526 Validation Loss: 1.6454\n",
      "Epoch 178/200, Train Loss: 1.3507 Validation Loss: 1.6379\n",
      "Epoch 179/200, Train Loss: 1.3510 Validation Loss: 1.6475\n",
      "Epoch 180/200, Train Loss: 1.3484 Validation Loss: 1.6453\n",
      "Epoch 181/200, Train Loss: 1.3476 Validation Loss: 1.6467\n",
      "Epoch 182/200, Train Loss: 1.3482 Validation Loss: 1.6581\n",
      "Epoch 183/200, Train Loss: 1.3453 Validation Loss: 1.6443\n",
      "Epoch 184/200, Train Loss: 1.3483 Validation Loss: 1.6450\n",
      "Epoch 185/200, Train Loss: 1.3481 Validation Loss: 1.6586\n",
      "Epoch 186/200, Train Loss: 1.3459 Validation Loss: 1.6555\n",
      "Epoch 187/200, Train Loss: 1.3473 Validation Loss: 1.6509\n",
      "Epoch 188/200, Train Loss: 1.3457 Validation Loss: 1.6564\n",
      "Epoch 189/200, Train Loss: 1.3446 Validation Loss: 1.6461\n",
      "Epoch 190/200, Train Loss: 1.3467 Validation Loss: 1.6478\n",
      "Epoch 191/200, Train Loss: 1.3433 Validation Loss: 1.6471\n",
      "Epoch 192/200, Train Loss: 1.3451 Validation Loss: 1.6559\n",
      "Epoch 193/200, Train Loss: 1.3456 Validation Loss: 1.6408\n",
      "Epoch 194/200, Train Loss: 1.3441 Validation Loss: 1.6499\n",
      "Epoch 195/200, Train Loss: 1.3412 Validation Loss: 1.6413\n",
      "Epoch 196/200, Train Loss: 1.3442 Validation Loss: 1.6506\n",
      "Epoch 197/200, Train Loss: 1.3433 Validation Loss: 1.6521\n",
      "Epoch 198/200, Train Loss: 1.3415 Validation Loss: 1.6478\n",
      "Epoch 199/200, Train Loss: 1.3414 Validation Loss: 1.6500\n",
      "Epoch 200/200, Train Loss: 1.3418 Validation Loss: 1.6480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(gptConfig.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in dls['train']:\n",
    "        x, y = x.to(gptConfig.device), y.to(gptConfig.device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=gptConfig.device, dtype=gptConfig.dtype):\n",
    "          logits = model(x)\n",
    "          loss = loss_func(logits.view(-1, gptConfig.vocab_size), y.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), gptConfig.max_grad_norm) # to clip gradients\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(), torch.autocast(device_type=gptConfig.device, dtype=gptConfig.dtype):\n",
    "        for x, y in dls['valid']:\n",
    "            x, y = x.to(gptConfig.device), y.to(gptConfig.device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits.view(-1, gptConfig.vocab_size), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(dls['train']):.4f} Validation Loss: {val_loss/len(dls['valid']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O06IQ_HjDC32",
   "metadata": {},
   "source": [
    "## Inferenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhlmny_i-16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be such a mind.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Help, most that hang pleasing, wer\n",
      "To please your forfections, strang\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, max_new_tokens=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    prompt: string to start generation\n",
    "    max_new_tokens: how many tokens to generate\n",
    "    temperature: higher = more random, lower = more deterministic\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim\n",
    "    tokens = tokens.to('cuda')\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to last seq_len tokens if needed\n",
    "        context = tokens if tokens.size(1) <= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "          logits = model(context)\n",
    "        logits = logits[:, -1, :] / temperature  # Focus on last token\n",
    "\n",
    "        # Sample next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append to sequence\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens.squeeze().tolist())\n",
    "print(generate(\"To be or not to be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ZtnLkQDGgqp",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this implementation, we built a GPT decoder model from scratch using PyTorch, gaining deep insights into transformer architecture. Starting with a character-level tokenizer on Shakespeare's complete works, we implemented every component: embeddings with positional encoding, causal masked multi-head attention, feed-forward networks with GELU activation, and residual connections with layer normalization.\n",
    "\n",
    "The final model contains 33,793 parameters across 8 decoder layers with 8 attention heads each, processing sequences of 128 tokens with an embedding dimension of 128. While significantly smaller than production models like GPT-2, this implementation demonstrates all the core concepts that make transformers powerful for language modeling.\n",
    "\n",
    "Key learnings include understanding how causal masking enables autoregressive generation, why residual connections and layer normalization are crucial for training deep networks, and how multi-head attention allows the model to attend to different aspects of the input simultaneously.\n",
    "\n",
    "This foundational implementation can be extended with techniques like dropout scheduling, gradient clipping, better initalization and larger and other architectures to improve performance further.\n",
    "\n",
    "Motivations:\n",
    "1. Another excelent intro can be found here done by the [3Blue1Brown](https://youtu.be/wjZofJX0v4M)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XtGgle2BJ6yS",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vaswani et al. (2017) - [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) - The original Transformer paper\n",
    "1. Radford et al. (2019) - [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - The GPT-2 paper\n",
    "1. Andrej Karpathy - [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) - Video tutorial\n",
    "1. Another excelent intro can be found here done by the [3Blue1Brown](https://youtu.be/wjZofJX0v4M).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
