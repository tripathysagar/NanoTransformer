{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6465fd42",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "description: ImageEncoder\n",
    "output-file: ImageEncoder.html\n",
    "title: ImageEncoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b724f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ImageEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f180a",
   "metadata": {
    "id": "724f180a"
   },
   "source": [
    "## Vision Encoder (Simple CNN)\n",
    "\n",
    "1. **Input**: Fashion MNIST images (bs, 1, 28, 28)\n",
    "1. 2-3 convolutional layers + pooling\n",
    "1. **Output**: Single feature vector (bs, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785ded0",
   "metadata": {
    "id": "d785ded0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558723e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "558723e7",
    "outputId": "ac8c3f82-022e-4cda-94a6-81908ef85802",
    "time_run": "5:11:51p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install -q git+https://github.com/tripathysagar/NanoTransformer.git\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a925a120",
   "metadata": {
    "id": "a925a120",
    "time_run": "5:12:04p"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from NanoTransformer.data import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80d26",
   "metadata": {
    "id": "d4c80d26"
   },
   "source": [
    "For creating the dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22071c75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22071c75",
    "outputId": "9d49712a-c0d5-415a-c1fe-138d31df40e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "dls = get_vision_classifier_dl()\n",
    "for x, y in dls['valid']:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580682f",
   "metadata": {
    "id": "c580682f"
   },
   "source": [
    "configs for device and dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516a5d9",
   "metadata": {
    "id": "2516a5d9"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VisionConfig:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16\n",
    "\n",
    "    lr = 1e-3\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    head_op_dim = 512\n",
    "    nc = 10\n",
    "\n",
    "visConfig = VisionConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3c4d7",
   "metadata": {
    "id": "fae3c4d7"
   },
   "source": [
    "### Model\n",
    "The classifier consists of 3 type of object\n",
    "1. **ResBlock**: For building a resblock\n",
    "1. **VisionEncoder**: which consists of vision head for classifier\n",
    "1. **classfier head**: helps in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a348b301",
   "metadata": {
    "id": "a348b301"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block with skip connection.\n",
    "    Applies two convolutions with a skip connection that adds input to output.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nf, ks=3, stride=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ni: number of input channels\n",
    "            nf: number of output channels (filters)\n",
    "            ks: kernel size (default 3)\n",
    "            stride: stride for first conv (default 2 for downsampling)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First conv: changes channels and spatial dims\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride),\n",
    "            nn.BatchNorm2d(nf))\n",
    "\n",
    "        # Second conv: keeps channels and spatial dims constant\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf, ks, padding=ks//2, stride=1),\n",
    "            nn.BatchNorm2d(nf))\n",
    "\n",
    "        # Handle dimension mismatch\n",
    "        self.skip = nn.Conv2d(ni, nf, 1, stride=stride) if ni != nf else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add skip connection to output of two convs\n",
    "        return F.relu(self.skip(x) + self.conv2(F.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854b2d8",
   "metadata": {
    "id": "2854b2d8",
    "use_thinking": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN encoder for Fashion MNIST images.\n",
    "    Progressively downsamples and increases channels to create feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.VisionHead = nn.Sequential(\n",
    "            ResBlock(1, 64, ks=7, stride=2),      # 28→14\n",
    "            ResBlock(64, 128, stride=2),          # 14→7\n",
    "            ResBlock(128, 256, stride=2),         # 7→4\n",
    "            ResBlock(256, visConfig.head_op_dim, stride=2),         # 4→2\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()                          # 512\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input images (bs, 1, 28, 28)\n",
    "        Returns:\n",
    "            feature vector (bs, 512)\n",
    "        \"\"\"\n",
    "        return self.VisionHead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413fb5a",
   "metadata": {
    "id": "6413fb5a",
    "use_thinking": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "classifier = nn.Sequential(\n",
    "    VisionEncoder(),\n",
    "    nn.Sequential(\n",
    "            nn.Linear(visConfig.head_op_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, visConfig.nc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fa241",
   "metadata": {
    "id": "915fa241"
   },
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf7441c",
   "metadata": {
    "id": "bbf7441c"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17b59ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f17b59ff",
    "outputId": "2643fe27-b21a-442a-bb69-41113369bf51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5929,  0.2011, -0.2210, -0.7946,  0.3044,  0.2682,  0.2516,  0.3780,\n",
       "         0.8079,  0.0866], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classifier(x)\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "teErxV0Rc8sI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teErxV0Rc8sI",
    "outputId": "ed17cab4-2828-4df4-98fa-0aa207ded6e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1094)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred.softmax(-1).argmax(-1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4240a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c4240a4",
    "outputId": "c64a6c16-9a31-4b40-ef75-30b8f132152d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4303, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a921a",
   "metadata": {
    "id": "ac1a921a"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1GQJLjMwdHsD",
   "metadata": {
    "id": "1GQJLjMwdHsD"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def log(*args):\n",
    "    print(f\"{args[0]}   \\t{args[1]:.4f}   \\t{args[2]:.4f}\\t\\t{args[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1228e5bf",
   "metadata": {
    "id": "1228e5bf"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def vision_encoder_train(model, epochs=10):\n",
    "    model = model.to(visConfig.device)\n",
    "    optimizer = AdamW(model.parameters(), lr=visConfig.lr)\n",
    "\n",
    "    print(f\"Epoch \\tTrain Loss \\tValid Loss \\taccurecy\")\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, y in dls['train']:\n",
    "            x, y = x.to(visConfig.device), y.to(visConfig.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):\n",
    "                logits = model(x)\n",
    "                loss = loss_func(logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), visConfig.max_grad_norm) # to clip gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad(), torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):\n",
    "            for x, y in dls['valid']:\n",
    "                x, y = x.to(visConfig.device), y.to(visConfig.device)\n",
    "\n",
    "                logits = model(x)\n",
    "                loss = loss_func(logits, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predicted = logits.softmax(-1).argmax(-1)\n",
    "                total_correct += (predicted == y).sum().item()\n",
    "                total_samples += y.size(0)\n",
    "\n",
    "        accurecy = total_correct / total_samples\n",
    "        log(epoch+1, train_loss/len(dls['train']), val_loss/len(dls['valid']), accurecy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dca8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "905dca8c",
    "outputId": "327060b0-04dd-4ce6-868f-2b7cdb889105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \tTrain Loss \tValid Loss \taccurecy\n",
      "1   \t0.4452   \t0.3378\t\t0.8728\n",
      "2   \t0.2998   \t0.2488\t\t0.9080\n",
      "3   \t0.2496   \t0.2210\t\t0.9178\n",
      "4   \t0.2149   \t0.1625\t\t0.9411\n",
      "5   \t0.1841   \t0.1398\t\t0.9501\n",
      "6   \t0.1607   \t0.1164\t\t0.9571\n",
      "7   \t0.1340   \t0.0999\t\t0.9644\n",
      "8   \t0.1126   \t0.0833\t\t0.9702\n",
      "9   \t0.0928   \t0.0622\t\t0.9776\n",
      "10   \t0.0827   \t0.0523\t\t0.9815\n",
      "11   \t0.0696   \t0.0634\t\t0.9762\n",
      "12   \t0.0604   \t0.0397\t\t0.9862\n",
      "13   \t0.0564   \t0.0533\t\t0.9813\n",
      "14   \t0.0481   \t0.0270\t\t0.9904\n",
      "15   \t0.0430   \t0.0317\t\t0.9895\n"
     ]
    }
   ],
   "source": [
    "vision_encoder_train(classifier, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8af0f",
   "metadata": {
    "id": "9da8af0f"
   },
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c183fee",
   "metadata": {
    "id": "4c183fee"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def save_model(fn='classfier.pth'):\n",
    "    torch.save(classifier, path/fn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
