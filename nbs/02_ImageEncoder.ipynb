{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724f180a",
   "metadata": {},
   "source": [
    "## Vision Encoder (Simple CNN)\n",
    "\n",
    "1. **Input**: Fashion MNIST images (bs, 1, 28, 28)\n",
    "1. 2-3 convolutional layers + pooling\n",
    "1. **Output**: Single feature vector (bs, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785ded0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558723e7",
   "metadata": {
    "time_run": "5:11:51p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: NanoTransformer\r\n",
      "Version: 0.0.1\r\n",
      "Summary: a transformer experiments\r\n",
      "Home-page: https://github.com/tripathysagar/NanoTransformer\r\n",
      "Author: tripathysagar\r\n",
      "Author-email: tripathysagar08@gmail.com\r\n",
      "License: Apache Software License 2.0\r\n",
      "Location: /app/data/.local/lib/python3.12/site-packages\r\n",
      "Editable project location: /app/data/NanoTransformer\r\n",
      "Requires: \r\n",
      "Required-by: \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install -q git+https://github.com/tripathysagar/NanoTransformer.git\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a925a120",
   "metadata": {
    "time_run": "5:12:04p"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from NanoTransformer.data import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80d26",
   "metadata": {},
   "source": [
    "For creating the dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22071c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "dls = get_vision_classifier_dl()\n",
    "for x, y in dls['valid']:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580682f",
   "metadata": {},
   "source": [
    "configs for device and dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2516a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VisionConfig:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16 # Use bfloat16 on Ampere+ GPUs, otherwise use float16\n",
    "\n",
    "    lr = 1e-3\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "visConfig = VisionConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3c4d7",
   "metadata": {},
   "source": [
    "### Model\n",
    "The classifier consists of 3 type of object\n",
    "1. **ResBlock**: For building a resblock \n",
    "1. **VisionEncoder**: which consists of vision head for classifier\n",
    "1. **classfier head**: helps in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a348b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block with skip connection.\n",
    "    Applies two convolutions with a skip connection that adds input to output.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nf, ks=3, stride=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ni: number of input channels\n",
    "            nf: number of output channels (filters)\n",
    "            ks: kernel size (default 3)\n",
    "            stride: stride for first conv (default 2 for downsampling)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First conv: changes channels and spatial dims\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride),\n",
    "            nn.BatchNorm2d(nf))\n",
    "        \n",
    "        # Second conv: keeps channels and spatial dims constant\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf, ks, padding=ks//2, stride=1),\n",
    "            nn.BatchNorm2d(nf))\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        self.skip = nn.Conv2d(ni, nf, 1, stride=stride) if ni != nf else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add skip connection to output of two convs\n",
    "        return F.relu(self.skip(x) + self.conv2(F.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2854b2d8",
   "metadata": {
    "use_thinking": true
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN encoder for Fashion MNIST images.\n",
    "    Progressively downsamples and increases channels to create feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.VisionHead = nn.Sequential(\n",
    "            ResBlock(1, 64, ks=7, stride=2),      # 28→14\n",
    "            ResBlock(64, 128, stride=2),          # 14→7\n",
    "            ResBlock(128, 256, stride=2),         # 7→4\n",
    "            ResBlock(256, 512, stride=2),         # 4→2\n",
    "            ResBlock(512, 512, stride=2),         # 2→2\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()                          # 512\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input images (bs, 1, 28, 28)\n",
    "        Returns:\n",
    "            feature vector (bs, 512)\n",
    "        \"\"\"\n",
    "        return self.VisionHead(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6413fb5a",
   "metadata": {
    "use_thinking": true
   },
   "outputs": [],
   "source": [
    "#|export \n",
    "classifier = nn.Sequential(\n",
    "    VisionEncoder(),\n",
    "    nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fa241",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf7441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17b59ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7743, -0.4576, -0.0745,  0.6165,  0.7950, -0.0955, -0.5897, -0.6505,\n",
       "        -0.7653,  0.1114], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classifier(x)\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4240a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4736, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a921a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def vision_encoder_train(model, epochs=10):\n",
    "    model = model.to(visConfig.device)\n",
    "    optimizer = AdamW(model.parameters(), lr=visConfig.lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, y in dls['train']:\n",
    "            x, y = x.to(visConfig.device), y.to(visConfig.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):\n",
    "                logits = model(x)\n",
    "                loss = loss_func(logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), visConfig.max_grad_norm) # to clip gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad(), torch.autocast(device_type=visConfig.device, dtype=visConfig.dtype):\n",
    "            for x, y in dls['valid']:\n",
    "                x, y = x.to(visConfig.device), y.to(visConfig.device)\n",
    "\n",
    "                logits = model(x)\n",
    "                loss = loss_func(logits, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(dls['train']):.4f} Validation Loss: {val_loss/len(dls['valid']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder_train(classifier, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8af0f",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c183fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "torch.save(classifier, path/'classfier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
