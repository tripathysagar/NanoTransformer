{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af0a70a4",
   "metadata": {},
   "source": [
    "---\n",
    "description: RoPE\n",
    "output-file: RoPE.html\n",
    "title: RoPE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd240f-9c0d-4f62-9993-19a244a06956",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b838bc8-494b-446b-a63f-4f69aefa9df0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install -q git+https://github.com/tripathysagar/NanoTransformer.git\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I-ySoi2HRGgm",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch \tTrain Loss \tValid Loss \taccurecy\n",
      "1   \t0.4402   \t0.3328\t\t0.8752\n",
      "2   \t0.2984   \t0.3232\t\t0.8819\n",
      "3   \t0.2481   \t0.2814\t\t0.8949\n",
      "4   \t0.2169   \t0.2831\t\t0.8988\n",
      "5   \t0.1857   \t0.2617\t\t0.9086\n"
     ]
    }
   ],
   "source": [
    "from NanoTransformer.data import *\n",
    "import NanoTransformer.GPTText2Text as gpt_module\n",
    "from NanoTransformer.ImageEncoder import *\n",
    "from NanoTransformer.MultiModal import *\n",
    "\n",
    "gptConfig = gpt_module.gptConfig\n",
    "\n",
    "\n",
    "# training vison encoder\n",
    "vision_encoder_train(classifier, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d91a08",
   "metadata": {},
   "source": [
    "The basic traansfomer gpt2 char-wise on shakespre text and extend it to use multi modal format to process for Fashion MNIST captioning. Used pytorch.\n",
    "```py\n",
    "#|export\n",
    "import torch\n",
    "from torch import nn\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        self.register_buffer('pos_ids', torch.arange(config.seq_len))  # for adding the postional encoding from 0 to seq_len - 1\n",
    "\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.pos_embed =  nn.Embedding(config.seq_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, x, start_idx=0):           #bs * seq_len\n",
    "        return self.embed(x) + self.pos_embed(self.pos_ids[start_idx:start_idx+x.size(1)])     #bs * seq_len * embedding_dim\n",
    "```\n",
    "\n",
    "The `positional encoding` used in the embedding layer have a fixed length i.e. `128`. It can be thought of absolute postional embedding rather than using enter token one(relative). It is brittle easy to break. The motivation for RoPE(Rotary Position Embeddings)\n",
    "\n",
    "**1. Relative vs Absolute Position**\n",
    "Your current learned embeddings tell the model \"this is position 5\" or \"this is position 127\" (absolute). But what's often more useful is knowing \"these two tokens are 3 positions apart\" (relative). RoPE naturally encodes relative positions through its rotation mechanism.\n",
    "\n",
    "**2. Length Generalization**\n",
    "You mentioned this‚Äîwith learned embeddings, if you train on sequences up to length 512, you literally don't have embeddings for position 513+. With RoPE, the rotation pattern continues smoothly, so the model can handle longer sequences at inference time (though not infinitely longer‚Äîthere are still limits).\n",
    "\n",
    "**3. How the rotation achieves this**\n",
    "Here's the clever part: when you rotate query vector Q by angle Œ∏_i and key vector K by angle Œ∏_j, their dot product (in attention) depends on (Œ∏_i - Œ∏_j), which is the *relative* distance between positions i and j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3f2e7",
   "metadata": {},
   "source": [
    "Yes explaint the matn behind\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! Let's look at the math. I'll break it down step by step.\n",
    "\n",
    "**The Core Idea: 2D Rotation**\n",
    "\n",
    "Imagine we have a 2D vector [x, y]. We can rotate it by angle Œ∏ using this rotation matrix:\n",
    "\n",
    "```\n",
    "[cos(Œ∏)  -sin(Œ∏)]   [x]\n",
    "[sin(Œ∏)   cos(Œ∏)]   [y]\n",
    "```\n",
    "\n",
    "For position `m`, RoPE rotates pairs of dimensions in your query/key vectors. The rotation angle depends on:\n",
    "- The position `m` (where the token is in the sequence)\n",
    "- Which dimension pair we're rotating (different pairs use different base frequencies)\n",
    "\n",
    "**The angle formula:**\n",
    "For dimension pair `i`, at position `m`:\n",
    "```\n",
    "Œ∏_m,i = m √ó Œ∏_i\n",
    "where Œ∏_i = 1 / (10000^(2i/d))\n",
    "```\n",
    "\n",
    "Here `d` is your embedding dimension, and `i` goes from 0, 1, 2... for each pair.\n",
    "\n",
    "**Key insight:** When computing attention between position m and position n, the dot product ends up depending on `(m-n) √ó Œ∏_i`, which is the *relative* distance!\n",
    "\n",
    "Does this make sense so far? Should I clarify any part before we move to how this looks in code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608355f",
   "metadata": {},
   "source": [
    "Can you clarify the math behind it ? how it is dependent\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great question! Let me show you why the dot product depends on relative position (m - n).\n",
    "\n",
    "**Setup:**\n",
    "- Query vector Q at position m gets rotated by angle: Œ∏_m = m √ó Œ∏_i\n",
    "- Key vector K at position n gets rotated by angle: Œ∏_n = n √ó Œ∏_i\n",
    "\n",
    "**For a 2D pair [q‚ÇÅ, q‚ÇÇ] and [k‚ÇÅ, k‚ÇÇ]:**\n",
    "\n",
    "After rotation:\n",
    "```\n",
    "Q_rotated = [q‚ÇÅ¬∑cos(m¬∑Œ∏) - q‚ÇÇ¬∑sin(m¬∑Œ∏), q‚ÇÅ¬∑sin(m¬∑Œ∏) + q‚ÇÇ¬∑cos(m¬∑Œ∏)]\n",
    "K_rotated = [k‚ÇÅ¬∑cos(n¬∑Œ∏) - k‚ÇÇ¬∑sin(n¬∑Œ∏), k‚ÇÅ¬∑sin(n¬∑Œ∏) + k‚ÇÇ¬∑cos(n¬∑Œ∏)]\n",
    "```\n",
    "\n",
    "**The dot product:**\n",
    "When you compute Q_rotated ¬∑ K_rotated and apply trigonometric identities (cos(A)cos(B) + sin(A)sin(B) = cos(A-B)), you get terms that depend on:\n",
    "\n",
    "```\n",
    "cos((m-n)¬∑Œ∏) and sin((m-n)¬∑Œ∏)\n",
    "```\n",
    "\n",
    "See it? The angles combine to give **(m - n)**, which is the relative distance between positions!\n",
    "\n",
    "This is the elegant part: the attention score naturally encodes \"how far apart are these tokens?\" rather than \"what absolute positions are they at?\"\n",
    "\n",
    "Does this click? Would you like me to work through the trigonometric identity step, or are you ready to think about implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca62c6",
   "metadata": {},
   "source": [
    "The angle thing is fixed right?\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good question! Let me clarify what's fixed and what varies:\n",
    "\n",
    "**Fixed:**\n",
    "- Œ∏·µ¢ (the base angle for dimension pair i) - this is fixed based on the formula Œ∏·µ¢ = 1 / (10000^(2i/d))\n",
    "- Different dimension pairs have different fixed Œ∏·µ¢ values\n",
    "\n",
    "**Varies:**\n",
    "- The actual rotation angle: **m √ó Œ∏·µ¢** - this changes based on which position m you're at\n",
    "\n",
    "So for example:\n",
    "- Dimension pair 0 has Œ∏‚ÇÄ = 1 (fixed)\n",
    "- At position 0: rotation angle = 0 √ó Œ∏‚ÇÄ = 0\n",
    "- At position 1: rotation angle = 1 √ó Œ∏‚ÇÄ = Œ∏‚ÇÄ\n",
    "- At position 5: rotation angle = 5 √ó Œ∏‚ÇÄ = 5Œ∏‚ÇÄ\n",
    "\n",
    "The pattern is fixed, but each position gets progressively more rotation.\n",
    "\n",
    "Does that distinction make sense? Is this what you were asking about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dac6c",
   "metadata": {},
   "source": [
    "explain the the formula once again\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Sure! Let's break down the formula:\n",
    "\n",
    "**Œ∏·µ¢ = 1 / (10000^(2i/d))**\n",
    "\n",
    "Where:\n",
    "- **d** = your embedding dimension (e.g., if your embedding is 128-dimensional, d = 128)\n",
    "- **i** = which dimension pair you're working on (i = 0, 1, 2, 3, ...)\n",
    "- **10000** = a constant (this gives a good range of frequencies)\n",
    "\n",
    "**What it does:**\n",
    "- For i = 0 (first pair): Œ∏‚ÇÄ = 1 / (10000^0) = 1 (fastest rotation)\n",
    "- For i = 1 (second pair): Œ∏‚ÇÅ = 1 / (10000^(2/d)) (slower rotation)\n",
    "- For i = 2 (third pair): Œ∏‚ÇÇ = 1 / (10000^(4/d)) (even slower)\n",
    "- ...and so on\n",
    "\n",
    "Each dimension pair rotates at a different speed. Lower dimension pairs rotate faster, higher ones rotate slower. This creates different \"wavelengths\" of position information, kind of like how Fourier analysis uses different frequencies.\n",
    "\n",
    "Does this clarify it? What part still seems confusing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4696c",
   "metadata": {},
   "source": [
    "The calculation of RoPE is done on Q and K matrix right. So it is taken on pair wise basis i.e. 64 pairs each for 128as context lenght.\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "I think there's a small mix-up here. Let me clarify:\n",
    "\n",
    "**RoPE operates on two different dimensions:**\n",
    "\n",
    "1. **Embedding dimension (d)**: If your embedding is 128-dimensional, you split it into d/2 = 64 pairs. Each pair gets rotated.\n",
    "\n",
    "2. **Sequence position (m)**: This is the position in your sequence (context length). If your context length is 512, you have positions m = 0, 1, 2, ..., 511.\n",
    "\n",
    "**How they combine:**\n",
    "- Each of the 64 dimension pairs has its own base angle Œ∏·µ¢\n",
    "- Each position m in your sequence uses rotation angle m √ó Œ∏·µ¢\n",
    "\n",
    "So for a token at position m=5 with embedding dim 128:\n",
    "- Dimension pair 0 gets rotated by 5 √ó Œ∏‚ÇÄ\n",
    "- Dimension pair 1 gets rotated by 5 √ó Œ∏‚ÇÅ\n",
    "- ... all 64 pairs get rotated\n",
    "\n",
    "Does this clarify the difference between embedding dimension and sequence position?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091ec32",
   "metadata": {},
   "source": [
    "it will be called on each Q and K ?\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Yes, exactly! RoPE is applied to both Q and K matrices (but **not** to V).\n",
    "\n",
    "Typically in your attention mechanism, after you compute Q, K, V from your input, you would:\n",
    "\n",
    "1. Apply RoPE to Q\n",
    "2. Apply RoPE to K  \n",
    "3. Leave V unchanged\n",
    "4. Then compute attention as usual: softmax(QK^T / sqrt(d)) √ó V\n",
    "\n",
    "Do you have your attention mechanism code handy? Where in your current code do you compute Q, K, and V?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e86afd",
   "metadata": {},
   "source": [
    "lets implemet rope\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! Let's implement RoPE step by step.\n",
    "\n",
    "First, let's create a function to compute the base angles Œ∏·µ¢ for each dimension pair.\n",
    "\n",
    "Can you write a function that:\n",
    "- Takes `d` (embedding dimension) as input\n",
    "- Returns a tensor of shape (d/2,) containing Œ∏·µ¢ = 1 / (10000^(2i/d)) for i = 0, 1, 2, ..., d/2-1\n",
    "\n",
    "Give it a try! What would you write?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841eee7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(20)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058f922",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.),\n",
       " tensor(0.3981),\n",
       " tensor(0.1585),\n",
       " tensor(0.0631),\n",
       " tensor(0.0251),\n",
       " tensor(0.0100),\n",
       " tensor(0.0040),\n",
       " tensor(0.0016),\n",
       " tensor(0.0006),\n",
       " tensor(0.0003),\n",
       " tensor(1.0000e-04),\n",
       " tensor(3.9811e-05),\n",
       " tensor(1.5849e-05),\n",
       " tensor(6.3096e-06),\n",
       " tensor(2.5119e-06),\n",
       " tensor(1.0000e-06),\n",
       " tensor(3.9811e-07),\n",
       " tensor(1.5849e-07),\n",
       " tensor(6.3096e-08),\n",
       " tensor(2.5119e-08)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 20\n",
    "def fun(i):\n",
    "    return 1/10000 ** (2*i/d)\n",
    "\n",
    "[fun(i) for i in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42da51c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.9811e-01, 1.5849e-01, 6.3096e-02, 2.5119e-02, 1.0000e-02,\n",
       "        3.9811e-03, 1.5849e-03, 6.3096e-04, 2.5119e-04, 1.0000e-04, 3.9811e-05,\n",
       "        1.5849e-05, 6.3096e-06, 2.5119e-06, 1.0000e-06, 3.9811e-07, 1.5849e-07,\n",
       "        6.3096e-08, 2.5119e-08])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = 1.0 / (10000 ** (2 * t / d))\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c34a2e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d7a27",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 3.9811e-01, 1.5849e-01, 6.3096e-02, 2.5119e-02, 1.0000e-02,\n",
       "         3.9811e-03, 1.5849e-03, 6.3096e-04, 2.5119e-04, 1.0000e-04, 3.9811e-05,\n",
       "         1.5849e-05, 6.3096e-06, 2.5119e-06, 1.0000e-06, 3.9811e-07, 1.5849e-07,\n",
       "         6.3096e-08, 2.5119e-08],\n",
       "        [2.0000e+00, 7.9621e-01, 3.1698e-01, 1.2619e-01, 5.0238e-02, 2.0000e-02,\n",
       "         7.9621e-03, 3.1698e-03, 1.2619e-03, 5.0238e-04, 2.0000e-04, 7.9621e-05,\n",
       "         3.1698e-05, 1.2619e-05, 5.0238e-06, 2.0000e-06, 7.9621e-07, 3.1698e-07,\n",
       "         1.2619e-07, 5.0238e-08]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angles = t[:,None] * theta\n",
    "angles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb7d6d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_angles = torch.cos(angles)\n",
    "sin_angles = torch.sin(angles)\n",
    "\n",
    "cos_angles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96e5b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, d_model = 1, 12, 10\n",
    "q = torch.randn((batch_size, seq_len, d_model))\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc20cd6",
   "metadata": {},
   "source": [
    " We want to go from shape (1, 12, 10) to (1, 12, 5, 2), where the last dimension is the pair [q‚ÇÅ, q‚ÇÇ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd28126",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pairs = q.view(batch_size, seq_len, d_model // 2, 2)\n",
    "q1, q2 = q_pairs[...,0], q_pairs[...,1]\n",
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea19ea",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_q1 = q1 * cos_angles[:seq_len, :5]  - q2 *sin_angles[:seq_len, :5]\n",
    "rotated_q2 = q1 * sin_angles[:seq_len, :5]  + q2 *cos_angles[:seq_len, :5]\n",
    "rotated_q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41125952",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 5, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_q = torch.stack([rotated_q1, rotated_q2], dim=-1)\n",
    "final_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefdd68e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ = final_q.view(batch_size, seq_len, -1)\n",
    "q_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122f3ba",
   "metadata": {},
   "source": [
    "## RoPE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a679e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def apply_rope(x):\n",
    "    bs, seq_len, emb_dim = x.shape\n",
    "\n",
    "    # Dimension pair indices as we are finding angles between emb_dim // 2 pairs\n",
    "    no = torch.arange(emb_dim // 2)\n",
    "    theta = 1.0 / (10000 ** (2 * no / emb_dim ))    # angle\n",
    "\n",
    "    # Position indices\n",
    "    positions = torch.arange(seq_len)\n",
    "    angles = positions[:, None] * theta             # (seq_len, emb_dim//2)\n",
    "    angles = angles.to(x.device)\n",
    "\n",
    "    cos_angles, sin_angles = torch.cos(angles), torch.sin(angles)\n",
    "\n",
    "    x = x.view(bs, seq_len, emb_dim // 2, 2)\n",
    "    x1, x2 = x[...,0], x[...,1]\n",
    "\n",
    "    assert x.shape[-1] == 2\n",
    "\n",
    "    rotated_x1 = x1 * cos_angles - x2 *sin_angles\n",
    "    rotated_x2 = x1 * sin_angles + x2 *cos_angles\n",
    "\n",
    "    assert rotated_x1.shape == rotated_x2.shape\n",
    "\n",
    "    x = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
    "\n",
    "    return x.view(bs, seq_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05071d7f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(apply_rope(q), q_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a782b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256, 128)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptConfig.embedding_dim, gptConfig.bs, gptConfig.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1dc39",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        no = torch.arange(head_dim // 2)\n",
    "        theta = 1.0 / (10000 ** (2 * no / head_dim ))    # angle\n",
    "\n",
    "        # Position indices\n",
    "        positions = torch.arange(seq_len)\n",
    "        angles = positions[:, None] * theta             # (seq_len, emb_dim//2)\n",
    "\n",
    "        self.register_buffer('cos_angles', torch.cos(angles))\n",
    "        self.register_buffer('sin_angles', torch.sin(angles))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, head_dim = x.shape # emb_dim is a name bug but it head_dim is passed\n",
    "\n",
    "        x = x.view(bs, seq_len, head_dim // 2, 2)\n",
    "        x1, x2 = x[..., 0], x[..., 1]\n",
    "\n",
    "        rotated_x1 = x1 * self.cos_angles[:seq_len] - x2 * self.sin_angles[:seq_len]\n",
    "        rotated_x2 = x1 * self.sin_angles[:seq_len] + x2 * self.cos_angles[:seq_len]\n",
    "\n",
    "        x = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
    "        return  x.view(bs, seq_len, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc481c6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All tests passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bs, seq_len, head_dim = 2, 10, 16\n",
    "x = torch.randn(bs, seq_len, head_dim)\n",
    "\n",
    "# Test 1: Shape preservation\n",
    "rope = RoPE(head_dim, seq_len)\n",
    "output = rope(x)\n",
    "assert output.shape == x.shape, \"Shape mismatch!\"\n",
    "\n",
    "# Test 2: Position 0 should be nearly unchanged (rotation by 0)\n",
    "x_pos0 = x[:, :1, :]  # First position only\n",
    "output_pos0 = rope(x_pos0)\n",
    "assert torch.allclose(x_pos0, output_pos0, atol=1e-6), \"Position 0 should be minimally rotated!\"\n",
    "\n",
    "# Test 3: Compare with apply_rope function\n",
    "output_fn = apply_rope(x)\n",
    "assert torch.allclose(output, output_fn, atol=1e-5), \"RoPE class and apply_rope don't match!\"\n",
    "\n",
    "print(\"‚úì All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c25e8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config:gpt_module.GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.embedding_dim % config.n_heads == 0\n",
    "        self.head_dim = config.embedding_dim // config.n_heads\n",
    "\n",
    "        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)\n",
    "\n",
    "        self.RoPE = RoPE(self.head_dim, config.seq_len)\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))\n",
    "        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building casual mask\n",
    "\n",
    "        self.dropout = nn.Dropout(p = config.dropout)\n",
    "\n",
    "    def forward(self, x): #bs * seq_len * embedding_dim\n",
    "\n",
    "        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)        #bs * seq_len * head_dim\n",
    "\n",
    "        Q, K = self.RoPE(Q), self.RoPE(K)\n",
    "\n",
    "        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -> bs * seq_len * seq_len\n",
    "\n",
    "        attn += self.mask[:x.shape[1], :x.shape[1]]\n",
    "\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        return self.dropout(attn @ V)         # bs * seq_len * seq_len @ bs * seq_len * head_dim -> bs * seq_len *  head_dim\n",
    "\n",
    "#monkey patching\n",
    "gpt_module.AttentionHead = AttentionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fa081",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (heads): ModuleList(\n",
       "    (0-7): 8 x AttentionHead(\n",
       "      (Q_W): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (K_W): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (V_W): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (RoPE): RoPE()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gpt_module.MultiHeadAttention(gptConfig)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa2b25",
   "metadata": {},
   "source": [
    "## Embedding without postional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c0590",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config:gpt_module.GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.embedding_dim) #token embedding\n",
    "\n",
    "    def forward(self, x):           #bs * seq_len\n",
    "        return self.embed(x)        #bs * seq_len * embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c4235",
   "metadata": {},
   "source": [
    "## Final Multi Modal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21503b2c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiModal(nn.Module):\n",
    "    def __init__(self):\n",
    "          super().__init__()\n",
    "\n",
    "          self.vis_encoder = classifier[0]\n",
    "          self.proj_layer = nn.Linear(visConfig.head_op_dim,  gptConfig.embedding_dim)\n",
    "\n",
    "          self.embed = Embedding(gptConfig)\n",
    "          self.blocks = nn.ModuleList(\n",
    "              [\n",
    "                  nn.Sequential(gpt_module.MultiHeadAttention(gptConfig), gpt_module.FFN(gptConfig))\n",
    "                  for _ in range(gptConfig.n_layers)\n",
    "              ])\n",
    "          self.layer_norm = nn.LayerNorm(gptConfig.embedding_dim)\n",
    "          self.lm_head = nn.Linear(gptConfig.embedding_dim, gptConfig.vocab_size)\n",
    "\n",
    "          # Ensure the vis_encoder is on the same device as the rest of the model\n",
    "          self.vis_encoder = self.vis_encoder.to(self.proj_layer.weight.device)\n",
    "\n",
    "          for param in self.vis_encoder.parameters():       #freezing the vision encoder\n",
    "              param.requires_grad = False\n",
    "\n",
    "    def forward(self, text_idx, image=None):\n",
    "        if image is not None:\n",
    "            # Ensure image has the correct dtype before passing to the encoder\n",
    "\n",
    "            image = image.to(self.proj_layer.weight.dtype)                     # ensure the image input has the correct data type\n",
    "            #print(f\"{image.shape=}\")\n",
    "            img_emb = self.proj_layer(self.vis_encoder(image)).unsqueeze(1)    # (bs, 1, 128)\n",
    "            text_emb = self.embed(text_idx)                                    # positions start at 1\n",
    "            #print(f\"{img_emb.shape=} {text_emb.shape=}\")\n",
    "\n",
    "            x = torch.cat([img_emb, text_emb], dim=1)\n",
    "            #print(f\"{x.shape=}\")\n",
    "        else:\n",
    "            x = self.embed(text_idx)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.lm_head(self.layer_norm(x))\n",
    "\n",
    "M = MultiModal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0cb68",
   "metadata": {},
   "source": [
    "## Dataloder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51605796-dc1d-46a4-a990-027af54bad7a",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a707f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader>,\n",
       " 'valid': <torch.utils.data.dataloader.DataLoader>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dl = get_text_dl()\n",
    "text_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8514d8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 128]), torch.Size([64, 128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x,y in text_dl['train']:\n",
    "    break\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28971b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "p = M(x[:1])\n",
    "assert list(p.shape) ,[1, gptConfig.seq_len, gptConfig.vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e6537-fe2a-4767-9609-31bd2fcd60b6",
   "metadata": {},
   "source": [
    "### Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721719e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 20)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_dl = get_mnist_caption_dl(tokenizer, path, 512)\n",
    "len(vision_dl['train']), len(vision_dl['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf9c28",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 1, 28, 28]), torch.Size([512, 19]), torch.Size([512, 20]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ims, inp, tgt = next(iter(vision_dl['train']))\n",
    "\n",
    "assert ims.shape[0] == inp.shape[0] == tgt.shape[0]\n",
    "assert ims.shape[1:] == torch.Size([1, 28, 28])\n",
    "ims.shape, inp.shape, tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FNzkMd6NSCV3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in M.parameters():\n",
    "  break\n",
    "i.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78355fc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 65])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = M(inp[:1], ims[:1])\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f54a64-e976-48bd-8737-6f17b82c1607",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "assert p.shape[0] == 1                               # BS\n",
    "assert p.shape[-1] == gptConfig.vocab_size\n",
    "assert p.shape[1] == tgt.shape[1]                    # output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mt8QYktNaeoW",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 65])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(M, inp, ims).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939644d7-2ea5-49c0-b21c-c9a42cb75030",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786dd16-343e-428e-87b8-ce831aa9f975",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = MultiModal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870059a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE exists in MultiHeadAttention heads: True\n"
     ]
    }
   ],
   "source": [
    "# Check if RoPE exists in the MultiModal model\n",
    "print(\"RoPE exists in MultiHeadAttention heads:\", any(hasattr(head, 'RoPE') for head in model.blocks[0][0].heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bu117mOVZtLM",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training : \n",
      "Text Loss: 4.2738 | Vision Loss: 4.5341\n",
      "------------------------------------------------------------\n",
      "0 -> 1.5189 : 0.9913\n",
      "1 -> 1.0851 : 0.8962\n",
      "2 -> 0.9936 : 0.8528\n",
      "3 -> 0.9432 : 0.8299\n",
      "4 -> 0.9110 : 0.8092\n",
      "5 -> 0.8872 : 0.7971\n",
      "6 -> 0.8688 : 0.7825\n",
      "7 -> 0.8541 : 0.7700\n",
      "8 -> 0.8427 : 0.7651\n",
      "9 -> 0.8323 : 0.7607\n",
      "10 -> 0.8246 : 0.7515\n",
      "11 -> 0.8166 : 0.7492\n",
      "12 -> 0.8101 : 0.7484\n",
      "13 -> 0.8044 : 0.7480\n",
      "14 -> 0.7989 : 0.7436\n",
      "15 -> 0.7940 : 0.7391\n",
      "16 -> 0.7901 : 0.7381\n",
      "17 -> 0.7855 : 0.7377\n",
      "18 -> 0.7820 : 0.7341\n",
      "19 -> 0.7782 : 0.7361\n",
      "20 -> 0.7752 : 0.7406\n",
      "21 -> 0.7723 : 0.7315\n",
      "22 -> 0.7692 : 0.7327\n",
      "23 -> 0.7672 : 0.7270\n",
      "24 -> 0.7645 : 0.7324\n",
      "25 -> 0.7622 : 0.7331\n",
      "26 -> 0.7600 : 0.7352\n",
      "27 -> 0.7583 : 0.7277\n",
      "28 -> 0.7561 : 0.7291\n",
      "29 -> 0.7538 : 0.7276\n",
      "------------------------------------------------------------\n",
      "after training : \n",
      "Text Loss: 1.3760 | Vision Loss: 0.1652\n"
     ]
    }
   ],
   "source": [
    "multi_modal_train(model, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EvcRS0j_hlY7",
   "metadata": {},
   "source": [
    "with the postional encoding the loss after training\n",
    "```text\n",
    "29 -> 0.7870 : 0.7611\n",
    "before training :\n",
    "Text Loss: 1.4400 | Vision Loss: 0.1656\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eTmVVpYZ4dX",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ims, inp, tgt = next(iter(vision_dl['valid']))\n",
    "\n",
    "idx = 20\n",
    "actual_caption = tokenizer.decode(inp[idx]).split('\\n')[0]  # as \\n is padded to the label\n",
    "pred_caption = generate_caption(model, ims[idx])\n",
    "\n",
    "show_images(ims[idx], f\"A:{actual_caption} P:{pred_caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pnGpPBCGfPwl",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('knitted top', 'knit top', [2], [2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_caption, pred_caption, find_lbl(actual_caption), find_lbl(pred_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_SBBt7Vufql3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, prompt, max_new_tokens=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    prompt: string to start generation\n",
    "    max_new_tokens: how many tokens to generate\n",
    "    temperature: higher = more random, lower = more deterministic\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim\n",
    "    tokens = tokens.to(multiConfig.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to last seq_len tokens if needed\n",
    "        context = tokens if tokens.size(1) <= gptConfig.seq_len else tokens[:, -gptConfig.seq_len:]\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.autocast(device_type=multiConfig.device, dtype=torch.bfloat16):\n",
    "            logits = model(context)\n",
    "        logits = logits[:, -1, :] / temperature  # Focus on last token\n",
    "\n",
    "        # Sample next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append to sequence\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6z6htn8fZ93",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be done but love had ends,\n",
      "Thou art a dain arms in service to him.\n",
      "Which, could that he was some mad farth.\n",
      "\n",
      "POLIXENES:\n",
      "A word, ''t, all be root.\n",
      "\n",
      "MENENIUS:\n",
      "Goubly Grumio son:\n",
      "'Thou depart of such feable, my whose fouls fault,\n",
      "And since most deceit for brother\n",
      "To be bold the usurp's foe that hate to thee,\n",
      "Say now to very rough to Juliet digniors, and the fair;\n",
      "And if not hold, who cannot death there.\n",
      "More wheful ssues? What laid have the world's up,\n",
      "When it disposition; I would awhile if\n",
      "They we call'd your f\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"To be or not to be\", max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I9etYHP5WhWn",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MQsUb3438EAt",
   "metadata": {},
   "source": [
    "## Better Init\n",
    "\n",
    "For GPT-style models, the most common initialization approach is:\n",
    "\n",
    "**1. Linear layers & Embeddings:**\n",
    "- Use normal distribution with mean=0, std=0.02 (GPT-2 standard)\n",
    "\n",
    "**2. Special scaling for residual connections:**\n",
    "- Scale down weights in layers that feed into residuals by 1/‚àö(2*num_layers)\n",
    "- num_layers = 4(gptConfig.n_layers)\n",
    "- This prevents activations from growing too large as you stack layers\n",
    "\n",
    "**3. Bias terms:**\n",
    "- Initialize to 0\n",
    "- no weight decay\n",
    "\n",
    "**4. Layer norms:**\n",
    "- Weight=1, bias=0 (usually PyTorch's default)\n",
    "- no weight deacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qDuaE8gbfgQg",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def _init_weight(self):\n",
    "  for name, module in self.named_modules():\n",
    "      if isinstance(module, nn.Linear):\n",
    "          module.weight.data.normal_(mean=0, std=0.02)\n",
    "          module.bias.data.zero_()\n",
    "\n",
    "          if 'linear' in name and 'linear1' not in name: # the residual wight is init for\n",
    "              module.weight.data *= 1./((2*gptConfig.n_layers) ** 0.5)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "          module.weight.data.normal_(mean=0, std=0.02)\n",
    "\n",
    "MultiModal._init_weight = _init_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MsaZG_5kMifE",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def decay_params(self):\n",
    "  no_decay_params, decay_params = [], []\n",
    "  for name, param in self.named_parameters():\n",
    "      if 'bias' in name or 'layer_norm' in name:\n",
    "          no_decay_params.append(param)\n",
    "      else:\n",
    "          decay_params.append(param)\n",
    "\n",
    "  return decay_params, no_decay_params\n",
    "\n",
    "MultiModal.decay_params = decay_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjNYJIIx-XP8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def multi_modal_train(model, epochs):\n",
    "    model = model.to(multiConfig.device)\n",
    "\n",
    "    print(f\"before training : \")\n",
    "    total_loss(model)\n",
    "\n",
    "    decay_params, no_decay_params = model.decay_params()\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {'params': decay_params, 'weight_decay': 0.1},\n",
    "        {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "    ], lr=multiConfig.lr)\n",
    "\n",
    "    print(\"---\"*20)\n",
    "    it = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, no_train = 0, 0\n",
    "        train_iters.reset()\n",
    "\n",
    "        for text_input, text_target, ims in train_iters: # Iterate directly over\n",
    "            it += 1\n",
    "            no_train += 1\n",
    "            # Handle None for images\n",
    "            if ims is not None:\n",
    "                ims = ims.to(multiConfig.device)\n",
    "\n",
    "            text_input, text_target = text_input.to(multiConfig.device), text_target.to(multiConfig.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=multiConfig.device, dtype=multiConfig.dtype):\n",
    "              logits = pred(model, text_input, ims) #model(text_input, ims)\n",
    "\n",
    "              loss = cal_loss(logits, text_target)    #loss_func(logits.reshape(-1, gptConfig.vocab_size), text_target.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), gptConfig.max_grad_norm) # to clip gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        valid_iters.reset()\n",
    "        model.eval()\n",
    "        val_loss, no_valid = 0, 0\n",
    "        with torch.no_grad(), torch.autocast(device_type=gptConfig.device, dtype=gptConfig.dtype):\n",
    "            for text_input, text_target, ims in valid_iters:\n",
    "                no_valid += 1\n",
    "\n",
    "                if ims is not None: ims = ims.to(multiConfig.device)\n",
    "\n",
    "                text_input, text_target = text_input.to(multiConfig.device), text_target.to(multiConfig.device)\n",
    "\n",
    "                logits = pred(model, text_input, ims)\n",
    "\n",
    "                loss = cal_loss(logits, text_target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"{epoch} -> {train_loss/no_train:.4f} : {val_loss/no_valid:.4f}\")\n",
    "    print(\"---\"*20)\n",
    "\n",
    "    print(f\"after training : \")\n",
    "    total_loss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsFdbMFzH_Mv",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training : \n",
      "Text Loss: 4.1801 | Vision Loss: 4.2309\n",
      "------------------------------------------------------------\n",
      "0 -> 1.7369 : 1.0681\n",
      "1 -> 1.1509 : 0.9375\n",
      "2 -> 1.0484 : 0.8886\n",
      "3 -> 0.9836 : 0.8652\n",
      "4 -> 0.9419 : 0.8346\n",
      "5 -> 0.9125 : 0.8180\n",
      "6 -> 0.8902 : 0.8025\n",
      "7 -> 0.8737 : 0.7991\n",
      "8 -> 0.8602 : 0.7858\n",
      "9 -> 0.8485 : 0.7717\n",
      "10 -> 0.8391 : 0.7735\n",
      "11 -> 0.8311 : 0.7724\n",
      "12 -> 0.8240 : 0.7590\n",
      "13 -> 0.8172 : 0.7537\n",
      "14 -> 0.8119 : 0.7523\n",
      "15 -> 0.8073 : 0.7502\n",
      "16 -> 0.8026 : 0.7466\n",
      "17 -> 0.7985 : 0.7501\n",
      "18 -> 0.7951 : 0.7431\n",
      "19 -> 0.7915 : 0.7401\n",
      "20 -> 0.7882 : 0.7548\n",
      "21 -> 0.7896 : 0.7399\n",
      "22 -> 0.7821 : 0.7385\n",
      "23 -> 0.7801 : 0.7371\n",
      "24 -> 0.7774 : 0.7365\n",
      "25 -> 0.7756 : 0.7391\n",
      "26 -> 0.7729 : 0.7327\n",
      "27 -> 0.7714 : 0.7354\n",
      "28 -> 0.7692 : 0.7326\n",
      "29 -> 0.7670 : 0.7314\n",
      "------------------------------------------------------------\n",
      "after training : \n",
      "Text Loss: 1.3982 | Vision Loss: 0.1666\n"
     ]
    }
   ],
   "source": [
    "model = MultiModal()\n",
    "model._init_weight()\n",
    "multi_modal_train(model, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kK_oDI4U85BI",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_caption = generate_caption(model, ims[idx])\n",
    "show_images(ims[idx], f\"A:{actual_caption} P:{pred_caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k5uzaxxi88Rt",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('knitted top', 'knit top', [2], [2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_caption, pred_caption, find_lbl(actual_caption), find_lbl(pred_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xoj11nmU8-Ul",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be heart's weathers' own.\n",
      "But possible heart neglect George,\n",
      "And there was France, which shall stand thou must;\n",
      "For his war, why unple forsweath flatter, and\n",
      "Fair pachses truthless wanting is to England's raw;\n",
      "For one to the king, carewing from of me? who\n",
      "sends himself of a worthy tlaze proceeding.\n",
      "\n",
      "COMINIUS:\n",
      "You have relex-place. Goat, my lord, madam:\n",
      "Stopp'd, forbear, we come of her your eyes:\n",
      "Take I so good done.\n",
      "\n",
      "CORIOLANUS:\n",
      "I am not.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Noble paid:\n",
      "For the lord truerein that caredentation,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"To be or not to be\", max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WoKv2PXqfrvr",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
